{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: visualisation of Train Loss and Validate Loss => done\n",
    "- TODO: table, maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and read files needed from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Countries lookup table\n",
    "lookup_df = pd.read_csv(\"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/country_latitude_longitude_area_lookup.csv\")\n",
    "# yield and production data\n",
    "yield_df = pd.read_csv(\"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/Yield_and_Production_data.csv\")\n",
    "# land cover data\n",
    "land_cover_df = pd.read_csv(\"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/Land_Cover_Percent_data.csv\")\n",
    "# define files to merge\n",
    "climate_files = {\n",
    "    \"rain\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/Rainf_tavg_data.csv\",\n",
    "    \"snow\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/Snowf_tavg_data.csv\",\n",
    "    \"esoil\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/ESoil_tavg_data.csv\",\n",
    "    \"soilmoisture_0_10\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilMoi0_10cm_inst_data.csv\",\n",
    "    \"soilmoisture_10_40\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilMoi10_40cm_inst_data.csv\",\n",
    "    \"soilmoisture_40_100\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilMoi40_100cm_inst_data.csv\",\n",
    "    \"soilmoisture_100_200\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilMoi100_200cm_inst_data.csv\",\n",
    "    \"soiltemp_0_10\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilTMP0_10cm_inst_data.csv\",\n",
    "    \"soiltemp_10_40\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilTMP10_40cm_inst_data.csv\",\n",
    "    \"soiltemp_40_100\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilTMP40_100cm_inst_data.csv\",\n",
    "    \"soiltemp_100_200\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilTMP100_200cm_inst_data.csv\",\n",
    "    \"tveg\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/TVeg_tavg_data.csv\",\n",
    "    \"tws\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/TWS_inst_data.csv\",\n",
    "    \"canopint\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/CanopInt_inst_data.csv\"  # Newly added dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial join and merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core dataset saved as Core_Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Helper Function: process and average by country-year\n",
    "def process_monthly_climate(filepath, variable_prefix, lookup_gdf):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Convert lat/lon into Shapely Points\n",
    "    df['geometry'] = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "    # Make GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "    # Reproject to metric system for distance calculation\n",
    "    gdf_proj = gdf.to_crs(\"EPSG:3857\")\n",
    "    lookup_proj = lookup_gdf.to_crs(\"EPSG:3857\")\n",
    "    # Spatial join to match each climate point to its nearest country lookup point\n",
    "    joined = gpd.sjoin_nearest(gdf_proj, lookup_proj, how='left', distance_col='distance')\n",
    "    # Drop geometry and return to original lat/lon projection\n",
    "    joined = joined.to_crs(\"EPSG:4326\").drop(columns='geometry')\n",
    "    # Identify monthly columns (e.g. month_1 to month_12)\n",
    "    month_cols = [col for col in df.columns if 'month' in col.lower()]\n",
    "    # Rename them to include the variable prefix (e.g. rain_1, rain_2, ...)\n",
    "    new_col_map = {col: f\"{variable_prefix}_{i+1}\" for i, col in enumerate(month_cols)}\n",
    "    joined = joined.rename(columns=new_col_map)\n",
    "    # Group by country + year and take the mean across all nearby points\n",
    "    monthly_avg = joined.groupby(['country', 'year'])[[*new_col_map.values()]].mean().reset_index()\n",
    "    return monthly_avg\n",
    "\n",
    "lookup_df['geometry'] = [Point(xy) for xy in zip(lookup_df['longitude'], lookup_df['latitude'])]\n",
    "lookup_gdf = gpd.GeoDataFrame(lookup_df, geometry='geometry', crs='EPSG:4326')\n",
    "# lowercase column names\n",
    "yield_df.columns = [c.strip().lower() for c in yield_df.columns]\n",
    "# filter to only 'Yield' rows\n",
    "yield_df = yield_df[yield_df[\"element\"] == \"Yield\"]\n",
    "# rename 'value' to 'yield'\n",
    "if \"yield\" not in yield_df.columns and \"value\" in yield_df.columns:\n",
    "    yield_df = yield_df.rename(columns={\"value\": \"yield\"})\n",
    "\n",
    "# Process and merge all climate datasets\n",
    "merged_climate = None\n",
    "for prefix, path in climate_files.items():\n",
    "    climate_df = process_monthly_climate(path, prefix, lookup_gdf)\n",
    "    if merged_climate is None:\n",
    "        merged_climate = climate_df\n",
    "    else:\n",
    "        # Outer join to accumulate all monthly features\n",
    "        merged_climate = pd.merge(merged_climate, climate_df, on=[\"country\", \"year\"], how=\"outer\")\n",
    "\n",
    "land_cover_df['geometry'] = [Point(xy) for xy in zip(land_cover_df['longitude'], land_cover_df['latitude'])]\n",
    "land_cover_gdf = gpd.GeoDataFrame(land_cover_df, geometry='geometry', crs='EPSG:4326')\n",
    "# Join each land cover point to nearest country point\n",
    "land_cov_joined = gpd.sjoin_nearest(land_cover_gdf.to_crs(\"EPSG:3857\"), lookup_gdf.to_crs(\"EPSG:3857\"), how='left')\n",
    "land_cov_joined = land_cov_joined.to_crs(\"EPSG:4326\").drop(columns='geometry')\n",
    "# Rename land cover class columns to standardized names\n",
    "land_class_cols = [col for col in land_cover_df.columns if \"class_\" in col.lower()]\n",
    "land_cov_col_map = {col: f\"mean_cov_{col.split('_')[-1]}\" for col in land_class_cols}\n",
    "land_cov_joined = land_cov_joined.rename(columns=land_cov_col_map)\n",
    "# Aggregate land cover by country (mean percent per class)\n",
    "land_cov_summary = land_cov_joined.groupby(\"country\")[list(land_cov_col_map.values())].mean().reset_index()\n",
    "\n",
    "# Merge with climate on country + year\n",
    "climate_with_land = pd.merge(merged_climate, land_cov_summary, on=\"country\", how=\"left\")\n",
    "\n",
    "# Merge with yield data \n",
    "final_df = pd.merge(yield_df, climate_with_land, on=[\"country\", \"year\"], how=\"inner\")\n",
    "\n",
    "# # Organize column order\n",
    "# def ordered_columns(final_df):\n",
    "#     rain_cols = [f\"rain_{i}\" for i in range(1, 13)]\n",
    "#     snow_cols = [f\"snow_{i}\" for i in range(1, 13)]\n",
    "#     esoil_cols = [f\"esoil_{i}\" for i in range(1, 13)]\n",
    "#     soilm_cols, soilt_cols = [], []\n",
    "#     for layer in [\"0_10\", \"10_40\", \"40_100\", \"100_200\"]:\n",
    "#         soilm_cols += [f\"soilmoisture_{layer}_{i}\" for i in range(1, 13)]\n",
    "#         soilt_cols += [f\"soiltemp_{layer}_{i}\" for i in range(1, 13)]\n",
    "\n",
    "#     tveg_cols = [f\"tveg_{i}\" for i in range(1, 13)]\n",
    "#     tws_cols = [f\"tws_{i}\" for i in range(1, 13)]\n",
    "#     canopint_cols = [f\"canopint_{i}\" for i in range(1, 13)]\n",
    "#     landcov_cols = [f\"mean_cov_{i}\" for i in range(1, 18)]\n",
    "\n",
    "#     base_cols = [\"country\", \"item\", \"year\"]\n",
    "#     # Combine all expected features (filter only those that exist in the final DataFrame)\n",
    "#     all_features = landcov_cols + rain_cols + snow_cols + esoil_cols + soilm_cols + soilt_cols + tveg_cols + tws_cols + canopint_cols\n",
    "#     all_features = [col for col in all_features if col in final_df.columns]\n",
    "\n",
    "#     return base_cols + all_features + [\"yield\"]\n",
    "\n",
    "def ordered_columns(final_df):\n",
    "    base_cols = [\"country\", \"item\", \"year\"]\n",
    "    landcov_cols = [f\"mean_cov_{i}\" for i in range(1, 18)]\n",
    "\n",
    "    def monthly_block(prefix):\n",
    "        # 12 monthly columns\n",
    "        months = [f\"{prefix}_{i}\" for i in range(1, 13)]\n",
    "        # Corresponding summary columns\n",
    "        summaries = [f\"{prefix}_{stat}\" for stat in ['mean', 'std', 'min', 'max']]\n",
    "        # Include only those that exist in the DataFrame\n",
    "        return [col for col in months + summaries if col in final_df.columns]\n",
    "\n",
    "    feature_blocks = []\n",
    "\n",
    "    # Add monthly + summary blocks for each\n",
    "    prefixes = [\n",
    "        \"rain\", \"snow\", \"esoil\",\n",
    "        \"soilmoisture_0_10\", \"soilmoisture_10_40\", \"soilmoisture_40_100\", \"soilmoisture_100_200\",\n",
    "        \"soiltemp_0_10\", \"soiltemp_10_40\", \"soiltemp_40_100\", \"soiltemp_100_200\",\n",
    "        \"tveg\", \"tws\", \"canopint\"\n",
    "    ]\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        feature_blocks.extend(monthly_block(prefix))\n",
    "\n",
    "    # Final ordered list\n",
    "    all_features = landcov_cols + feature_blocks\n",
    "    all_features = [col for col in all_features if col in final_df.columns]\n",
    "\n",
    "    return base_cols + all_features + [\"yield\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adding summary\n",
    "# Function to add summary stats for 12-month feature groups\n",
    "def add_monthly_summary_features(df, feature_prefixes):\n",
    "    for prefix in feature_prefixes:\n",
    "        month_cols = [f\"{prefix}_{i}\" for i in range(1, 13) if f\"{prefix}_{i}\" in df.columns]\n",
    "        if len(month_cols) == 12:  # only compute if all 12 months are present\n",
    "            df[f\"{prefix}_mean\"] = df[month_cols].mean(axis=1)\n",
    "            df[f\"{prefix}_std\"] = df[month_cols].std(axis=1)\n",
    "            df[f\"{prefix}_min\"] = df[month_cols].min(axis=1)\n",
    "            df[f\"{prefix}_max\"] = df[month_cols].max(axis=1)\n",
    "    return df\n",
    "\n",
    "monthly_prefixes = [\n",
    "    \"rain\", \"snow\", \"esoil\",\n",
    "    \"soilmoisture_0_10\", \"soilmoisture_10_40\", \"soilmoisture_40_100\", \"soilmoisture_100_200\",\n",
    "    \"soiltemp_0_10\", \"soiltemp_10_40\", \"soiltemp_40_100\", \"soiltemp_100_200\",\n",
    "    \"tveg\", \"tws\", \"canopint\"\n",
    "]\n",
    "\n",
    "final_df = add_monthly_summary_features(final_df, monthly_prefixes)\n",
    "\n",
    "# # Update output column list\n",
    "# summary_cols = [col for col in final_df.columns if any(stat in col for stat in [\"_mean\", \"_std\", \"_min\", \"_max\"])]\n",
    "# final_df = final_df[ordered_columns(final_df) + summary_cols]\n",
    "\n",
    "final_df = final_df[ordered_columns(final_df)]\n",
    "\n",
    "# Save final output\n",
    "final_df.to_csv(\"Core_Dataset.csv\", index=False)\n",
    "print(\"Core dataset saved as Core_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"Core_Dataset.csv\")\n",
    "\n",
    "# Move 'year' to the front\n",
    "cols = df.columns.tolist()\n",
    "if 'year' in cols:\n",
    "    cols.insert(0, cols.pop(cols.index('year')))\n",
    "df = df[cols]\n",
    "\n",
    "# Display structure\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert year to integer format (e.g. 2010, 2011, ...)\n",
    "df['year_raw'] = df['year'].astype(int)  # backup for clean export\n",
    "# Optional sanity check\n",
    "print(\"Years in dataset:\", sorted(df['year'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift yield by -1 to make next year's yield the prediction target\n",
    "df = df.sort_values(['country', 'item', 'year_raw'])  # ensure proper chronological order\n",
    "df['target'] = df.groupby(['country', 'item'])['yield'].shift(-1)\n",
    "\n",
    "# Drop rows where target is missing (i.e., last year of each country/item group)\n",
    "df = df.dropna(subset=['target']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_aware_split(df, test_year, val_year):\n",
    "    train_list, val_list, test_list = [], [], []\n",
    "\n",
    "    for _, group in df.groupby(['country', 'item']):\n",
    "        group = group.sort_values('year_raw')\n",
    "        train = group[group['year_raw'] < val_year]\n",
    "        val = group[group['year_raw'] == val_year]\n",
    "        test = group[group['year_raw'] == test_year]\n",
    "        train_list.append(train)\n",
    "        val_list.append(val)\n",
    "        test_list.append(test)\n",
    "\n",
    "    return pd.concat(train_list).reset_index(drop=True), \\\n",
    "           pd.concat(val_list).reset_index(drop=True), \\\n",
    "           pd.concat(test_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode country\n",
    "country_encoder = LabelEncoder()\n",
    "df['country_id'] = country_encoder.fit_transform(df['country'])\n",
    "num_countries = df['country_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode year as categorical ID for embedding\n",
    "year_encoder = LabelEncoder()\n",
    "df['year_id'] = year_encoder.fit_transform(df['year_raw'])  # not year+1, just original year\n",
    "num_years = df['year_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = time_aware_split(df, test_year=2021, val_year=2020)\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Val:\", val_df.shape)\n",
    "print(\"Test:\", test_df.shape)\n",
    "print(\"train export to csv\", train_df.to_csv(\"train.csv\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'item'\n",
    "df = pd.get_dummies(df, columns=['item'], prefix='item')\n",
    "item_onehot_cols = [col for col in df.columns if col.startswith('item_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded Sample:\")\n",
    "print(df[['country', 'country_id', 'year_raw', 'year_id']].drop_duplicates().sort_values('year_raw').head())\n",
    "\n",
    "print(f\"Total countries: {num_countries}\")\n",
    "print(f\"Total years: {num_years}\")\n",
    "print(f\"One-hot item columns: {item_onehot_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_onehot_cols = [col for col in df.columns if col.startswith('item_')]\n",
    "\n",
    "# Re-add any missing one-hot columns in val/test with zeros\n",
    "for col in item_onehot_cols:\n",
    "    for dset in [df]:\n",
    "        if col not in dset.columns:\n",
    "            dset[col] = 0\n",
    "\n",
    "# Columns to exclude from scaling\n",
    "exclude = ['year','yield', 'target', 'country', 'country_id', 'year_raw', 'year_id']\n",
    "exclude += item_onehot_cols  # exclude one-hot item columns too\n",
    "\n",
    "# Features to scale (everything else)\n",
    "feature_cols = [col for col in df.columns if col not in exclude]\n",
    "print(\"feature cols\",feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only on train\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "\n",
    "# Transform val and test\n",
    "val_df[feature_cols] = scaler.transform(val_df[feature_cols])\n",
    "test_df[feature_cols] = scaler.transform(test_df[feature_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the Target Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "train_df['target_scaled'] = target_scaler.fit_transform(train_df[['target']])\n",
    "val_df['target_scaled'] = target_scaler.transform(val_df[['target']])\n",
    "test_df['target_scaled'] = target_scaler.transform(test_df[['target']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fill any NaNs (if any exist after merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].mean())\n",
    "val_df[feature_cols] = val_df[feature_cols].fillna(train_df[feature_cols].mean())  # use train stats\n",
    "test_df[feature_cols] = test_df[feature_cols].fillna(train_df[feature_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature sample after scaling:\")\n",
    "print(train_df[feature_cols].head())\n",
    "print(\"Target range (train):\", train_df['target_scaled'].min(), train_df['target_scaled'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols += item_onehot_cols  # include one-hot columns last (will not scale them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. One-hot encode 'item' again, now in each split\n",
    "train_df = pd.get_dummies(train_df, columns=['item'], prefix='item')\n",
    "val_df = pd.get_dummies(val_df, columns=['item'], prefix='item')\n",
    "test_df = pd.get_dummies(test_df, columns=['item'], prefix='item')\n",
    "\n",
    "# 2. Ensure all splits have the same item_* columns\n",
    "all_item_cols = sorted(set(train_df.columns) | set(val_df.columns) | set(test_df.columns))\n",
    "item_onehot_cols = [col for col in all_item_cols if col.startswith(\"item_\")]\n",
    "\n",
    "for col in item_onehot_cols:\n",
    "    for dset in [train_df, val_df, test_df]:\n",
    "        if col not in dset.columns:\n",
    "            dset[col] = 0\n",
    "\n",
    "# 3. Reorder to maintain consistent column structure\n",
    "train_df = train_df.reindex(columns=sorted(train_df.columns))\n",
    "val_df = val_df.reindex(columns=sorted(val_df.columns))\n",
    "test_df = test_df.reindex(columns=sorted(test_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the list of features\n",
    "exclude = ['yield', 'target', 'country', 'year', 'year_raw', 'country_id', 'year_id']\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude and not col.startswith('item_')]\n",
    "feature_cols += item_onehot_cols  # now safe to add\n",
    "\n",
    "print(\"Final feature columns:\", feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all features are numeric floats\n",
    "train_df[feature_cols] = train_df[feature_cols].apply(pd.to_numeric, errors='coerce').astype('float32')\n",
    "val_df[feature_cols] = val_df[feature_cols].apply(pd.to_numeric, errors='coerce').astype('float32')\n",
    "test_df[feature_cols] = test_df[feature_cols].apply(pd.to_numeric, errors='coerce').astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train_num = torch.tensor(train_df[feature_cols].values, dtype=torch.float32)\n",
    "X_train_country = torch.tensor(train_df['country_id'].values, dtype=torch.long)\n",
    "X_train_year = torch.tensor(train_df['year_id'].values, dtype=torch.long)\n",
    "y_train = torch.tensor(train_df['target_scaled'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_val_num = torch.tensor(val_df[feature_cols].values, dtype=torch.float32)\n",
    "X_val_country = torch.tensor(val_df['country_id'].values, dtype=torch.long)\n",
    "X_val_year = torch.tensor(val_df['year_id'].values, dtype=torch.long)\n",
    "y_val = torch.tensor(val_df['target_scaled'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test_num = torch.tensor(test_df[feature_cols].values, dtype=torch.float32)\n",
    "X_test_country = torch.tensor(test_df['country_id'].values, dtype=torch.long)\n",
    "X_test_year = torch.tensor(test_df['year_id'].values, dtype=torch.long)\n",
    "y_test = torch.tensor(test_df['target_scaled'].values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "\n",
    "# # 1. Label encode country + year for embeddings\n",
    "# df['country_id'] = LabelEncoder().fit_transform(df['country'])\n",
    "# df['year_id'] = LabelEncoder().fit_transform(df['year_raw'])\n",
    "# num_countries = df['country_id'].nunique()\n",
    "# num_years = df['year_id'].nunique()\n",
    "\n",
    "# # 2. One-hot encode 'item' after split (to retain original for groupby)\n",
    "# train_df, val_df, test_df = time_aware_split(df, test_year=2021, val_year=2020)\n",
    "\n",
    "# for dset in [train_df, val_df, test_df]:\n",
    "#     dummies = pd.get_dummies(dset['item'], prefix='item')\n",
    "#     dset.drop(columns=['item'], inplace=True)\n",
    "#     dset[dummies.columns] = dummies\n",
    "\n",
    "# # 3. Align one-hot columns across splits\n",
    "# all_item_cols = sorted(set(col for d in [train_df, val_df, test_df] for col in d.columns if col.startswith(\"item_\")))\n",
    "# for col in all_item_cols:\n",
    "#     for dset in [train_df, val_df, test_df]:\n",
    "#         if col not in dset:\n",
    "#             dset[col] = 0\n",
    "\n",
    "# # 4. Determine features to scale\n",
    "# exclude = ['yield', 'target', 'country', 'year', 'year_raw', 'country_id', 'year_id']\n",
    "# feature_cols = [col for col in train_df.columns if col not in exclude and not col.startswith(\"item_\")]\n",
    "# feature_cols += all_item_cols  # One-hot columns added last (won’t be scaled)\n",
    "\n",
    "# # 5. Scale numeric features only (excluding one-hot columns)\n",
    "# scaler = MinMaxScaler()\n",
    "# train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "# val_df[feature_cols] = scaler.transform(val_df[feature_cols])\n",
    "# test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "# # 6. Fill NaNs if any\n",
    "# for dset in [train_df, val_df, test_df]:\n",
    "#     dset[feature_cols] = dset[feature_cols].fillna(train_df[feature_cols].mean())\n",
    "\n",
    "# # 7. Scale target\n",
    "# target_scaler = MinMaxScaler()\n",
    "# train_df['target_scaled'] = target_scaler.fit_transform(train_df[['target']])\n",
    "# val_df['target_scaled'] = target_scaler.transform(val_df[['target']])\n",
    "# test_df['target_scaled'] = target_scaler.transform(test_df[['target']])\n",
    "\n",
    "# # 8. Ensure numeric dtype for PyTorch\n",
    "# for dset in [train_df, val_df, test_df]:\n",
    "#     dset[feature_cols] = dset[feature_cols].astype('float32')\n",
    "\n",
    "# # 9. Convert to PyTorch tensors\n",
    "# def to_tensor(dset):\n",
    "#     X_num = torch.tensor(dset[feature_cols].values, dtype=torch.float32)\n",
    "#     X_country = torch.tensor(dset['country_id'].values, dtype=torch.long)\n",
    "#     X_year = torch.tensor(dset['year_id'].values, dtype=torch.long)\n",
    "#     y = torch.tensor(dset['target_scaled'].values, dtype=torch.float32).view(-1, 1)\n",
    "#     return X_num, X_country, X_year, y\n",
    "\n",
    "# X_train_num, X_train_country, X_train_year, y_train = to_tensor(train_df)\n",
    "# X_val_num, X_val_country, X_val_year, y_val = to_tensor(val_df)\n",
    "# X_test_num, X_test_country, X_test_year, y_test = to_tensor(test_df)\n",
    "\n",
    "# # Optional sanity check\n",
    "# print(f\"✅ Feature shape: {X_train_num.shape}, Country IDs: {num_countries}, Years: {num_years}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_num, X_train_country, X_train_year, y_train),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "class MLPWithEmbeddings(nn.Module):\n",
    "    def __init__(self, input_dim, num_countries, num_years, emb_dim_country=8, emb_dim_year=4):\n",
    "        super().__init__()\n",
    "        self.country_emb = nn.Embedding(num_countries, emb_dim_country)\n",
    "        self.year_emb = nn.Embedding(num_years, emb_dim_year)\n",
    "\n",
    "        total_input_dim = input_dim + emb_dim_country + emb_dim_year\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(total_input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Clamp output\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_country, x_year):\n",
    "        c_vec = self.country_emb(x_country)\n",
    "        y_vec = self.year_emb(x_year)\n",
    "        x = torch.cat([x_num, c_vec, y_vec], dim=1)\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "model = MLPWithEmbeddings(\n",
    "    input_dim=X_train_num.shape[1],\n",
    "    num_countries=num_countries,\n",
    "    num_years=num_years,\n",
    "    emb_dim_country=8,\n",
    "    emb_dim_year=4\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "# --- Training with Validation ---\n",
    "train_losses, train_maes, val_losses, val_maes = [], [], [], []\n",
    "\n",
    "# --- Early stopping setup ---\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss, total_train_mae = 0.0, 0.0\n",
    "    for xb_num, xb_country, xb_year, yb in train_loader:\n",
    "        pred = model(xb_num, xb_country, xb_year)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * xb_num.size(0)\n",
    "        total_train_mae += mean_absolute_error(yb.detach().numpy(), pred.detach().numpy()) * xb_num.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    avg_train_mae = total_train_mae / len(train_loader.dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_maes.append(avg_train_mae)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss, total_val_mae = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_country, xb_year, yb in DataLoader(\n",
    "            TensorDataset(X_val_num, X_val_country, X_val_year, y_val),\n",
    "            batch_size=32,\n",
    "            shuffle=False\n",
    "        ):\n",
    "            pred = model(xb_num, xb_country, xb_year)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            total_val_loss += loss.item() * xb_num.size(0)\n",
    "            total_val_mae += mean_absolute_error(yb.numpy(), pred.numpy()) * xb_num.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(y_val)\n",
    "    avg_val_mae = total_val_mae / len(y_val)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_maes.append(avg_val_mae)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Train MAE: {avg_train_mae:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val MAE: {avg_val_mae:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        best_model_state = model.state_dict()  # Save best model so far\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"  🔁 No improvement for {counter} epoch(s).\")\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"\\n⏹️ Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "    scheduler.step(avg_val_loss)\n",
    "    # Get and print current LR\n",
    "    current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    print(f\"📉 Learning rate after epoch {epoch+1}: {current_lr:.6f}\")\n",
    "# --- Restore best model ---\n",
    "model.load_state_dict(best_model_state)\n",
    "print(\"✅ Best model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With parameter sweep to see optimal values of country and year embeddings (current: 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='o')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test_num, X_test_country, X_test_year).numpy().flatten()  # predicted (scaled)\n",
    "    y_true_test = y_test.numpy().flatten()                                          # true (scaled)\n",
    "\n",
    "# Inverse transform to original yield scale\n",
    "y_pred_rescaled = target_scaler.inverse_transform(y_pred_test.reshape(-1, 1)).flatten()\n",
    "y_true_rescaled = target_scaler.inverse_transform(y_true_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_true_rescaled, y_pred_rescaled)\n",
    "print(f\"\\n🧪 Test MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# Calculate Test Loss and Test MAE\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Predict on the test set\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test_num, X_test_country, X_test_year).numpy().flatten()  # predicted (scaled)\n",
    "    y_true_test = y_test.numpy().flatten()                                          # true (scaled)\n",
    "\n",
    "# Calculate Test Loss (MSE)\n",
    "test_loss = mean_squared_error(y_true_test, y_pred_test)\n",
    "\n",
    "# Calculate Test MAE\n",
    "test_mae = mean_absolute_error(y_true_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Prediction range:\", y_pred_test.min(), \"to\", y_pred_test.max())\n",
    "print(\"✅ True yield range:\", y_true_test.min(), \"to\", y_true_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true_test, y_pred_test, alpha=0.6)\n",
    "plt.plot([y_true_test.min(), y_true_test.max()],\n",
    "         [y_true_test.min(), y_true_test.max()],\n",
    "         'r--', label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Yield\")\n",
    "plt.ylabel(\"Predicted Yield\")\n",
    "plt.title(\"Predicted vs Actual Yield (Test Set)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Recover the item name from one-hot columns\n",
    "item_cols = [col for col in test_df.columns if col.startswith('item_')]\n",
    "test_df['item'] = test_df[item_cols].idxmax(axis=1).str.replace('item_', '')\n",
    "\n",
    "# 2. Restore predictions and ground truth\n",
    "test_df['yield_true'] = y_true_rescaled\n",
    "test_df['yield_pred'] = y_pred_rescaled\n",
    "\n",
    "# 3. Select clean columns for display\n",
    "results_table = test_df[['year_raw', 'country', 'item', 'yield_true', 'yield_pred']]\n",
    "results_table = results_table.rename(columns={'year_raw': 'year'})\n",
    "\n",
    "# 4. Preview the output\n",
    "print(results_table.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.to_csv(\"yield_predictions.csv\", index=False)\n",
    "print(\"✅ Results saved to 'yield_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true_rescaled, y_pred_rescaled)\n",
    "plt.plot([y_true_rescaled.min(), y_true_rescaled.max()],\n",
    "         [y_true_rescaled.min(), y_true_rescaled.max()],\n",
    "         'r--', label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Yield\")\n",
    "plt.ylabel(\"Predicted Yield\")\n",
    "plt.title(\"Predicted vs Actual Yield (Test Set)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['yield_true'] = y_true_rescaled\n",
    "test_df['yield_pred'] = y_pred_rescaled\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "year_mae = test_df.groupby('year_raw').apply(\n",
    "    lambda x: mean_absolute_error(x['yield_true'], x['yield_pred'])\n",
    ").reset_index(name='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(year_mae['year_raw'], year_mae['mae'], marker='o', linestyle='-')\n",
    "plt.title(\"Test MAE by Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean Absolute Error (Yield)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts = test_df['year_raw'].value_counts().sort_index()\n",
    "year_mae['count'] = year_counts.values\n",
    "print(year_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "errors = np.abs(y_pred_rescaled.flatten() - y_true_rescaled.flatten())\n",
    "top_outliers_idx = np.argsort(errors)[-2:]  # indices of top 2 worst predictions\n",
    "\n",
    "# Print the values for inspection\n",
    "for idx in top_outliers_idx:\n",
    "    print(f\"[{idx}] Actual: {y_true_rescaled[idx]:,.0f}, Predicted: {y_pred_rescaled[idx]:,.0f}, Error: {errors[idx]:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors = pd.DataFrame({\n",
    "    \"actual\": y_true_rescaled,\n",
    "    \"predicted\": y_pred_rescaled,\n",
    "    \"error\": errors,\n",
    "    \"country\": test_df[\"country\"].values,\n",
    "    \"item\": test_df[\"item\"].values,\n",
    "    \"year\": test_df[\"year_raw\"].values\n",
    "})\n",
    "print(df_errors.sort_values(\"error\", ascending=False).head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
