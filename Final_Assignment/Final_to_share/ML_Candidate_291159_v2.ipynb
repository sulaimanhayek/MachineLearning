{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: visualisation of Train Loss and Validate Loss => done\n",
    "- TODO: table, maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and read files needed from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Countries lookup table\n",
    "lookup_df = pd.read_csv(\"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/country_latitude_longitude_area_lookup.csv\")\n",
    "# yield and production data\n",
    "yield_df = pd.read_csv(\"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/Yield_and_Production_data.csv\")\n",
    "# land cover data\n",
    "land_cover_df = pd.read_csv(\"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/Land_Cover_Percent_data.csv\")\n",
    "# define files to merge\n",
    "climate_files = {\n",
    "    \"rain\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/Rainf_tavg_data.csv\",\n",
    "    \"snow\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/Snowf_tavg_data.csv\",\n",
    "    \"esoil\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/ESoil_tavg_data.csv\",\n",
    "    \"soilmoisture_0_10\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilMoi0_10cm_inst_data.csv\",\n",
    "    \"soilmoisture_10_40\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilMoi10_40cm_inst_data.csv\",\n",
    "    \"soilmoisture_40_100\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilMoi40_100cm_inst_data.csv\",\n",
    "    \"soilmoisture_100_200\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilMoi100_200cm_inst_data.csv\",\n",
    "    \"soiltemp_0_10\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilTMP0_10cm_inst_data.csv\",\n",
    "    \"soiltemp_10_40\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilTMP10_40cm_inst_data.csv\",\n",
    "    \"soiltemp_40_100\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilTMP40_100cm_inst_data.csv\",\n",
    "    \"soiltemp_100_200\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/SoilTMP100_200cm_inst_data.csv\",\n",
    "    \"tveg\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/TVeg_tavg_data.csv\",\n",
    "    \"tws\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/TWS_inst_data.csv\",\n",
    "    \"canopint\": \"/Users/suli/Documents/source/repo/MachineLearning/Final_Assignment/ML 2025 Coursework Dataset - A1/CanopInt_inst_data.csv\"  # Newly added dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial join and merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core dataset saved as Core_Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Helper Function: process and average by country-year\n",
    "def process_monthly_climate(filepath, variable_prefix, lookup_gdf):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Convert lat/lon into Shapely Points\n",
    "    df['geometry'] = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "    # Make GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "    # Reproject to metric system for distance calculation\n",
    "    gdf_proj = gdf.to_crs(\"EPSG:3857\")\n",
    "    lookup_proj = lookup_gdf.to_crs(\"EPSG:3857\")\n",
    "    # Spatial join to match each climate point to its nearest country lookup point\n",
    "    joined = gpd.sjoin_nearest(gdf_proj, lookup_proj, how='left', distance_col='distance')\n",
    "    # Drop geometry and return to original lat/lon projection\n",
    "    joined = joined.to_crs(\"EPSG:4326\").drop(columns='geometry')\n",
    "    # Identify monthly columns (e.g. month_1 to month_12)\n",
    "    month_cols = [col for col in df.columns if 'month' in col.lower()]\n",
    "    # Rename them to include the variable prefix (e.g. rain_1, rain_2, ...)\n",
    "    new_col_map = {col: f\"{variable_prefix}_{i+1}\" for i, col in enumerate(month_cols)}\n",
    "    joined = joined.rename(columns=new_col_map)\n",
    "    # Group by country + year and take the mean across all nearby points\n",
    "    monthly_avg = joined.groupby(['country', 'year'])[[*new_col_map.values()]].mean().reset_index()\n",
    "    return monthly_avg\n",
    "\n",
    "lookup_df['geometry'] = [Point(xy) for xy in zip(lookup_df['longitude'], lookup_df['latitude'])]\n",
    "lookup_gdf = gpd.GeoDataFrame(lookup_df, geometry='geometry', crs='EPSG:4326')\n",
    "# lowercase column names\n",
    "yield_df.columns = [c.strip().lower() for c in yield_df.columns]\n",
    "# filter to only 'Yield' rows\n",
    "yield_df = yield_df[yield_df[\"element\"] == \"Yield\"]\n",
    "# rename 'value' to 'yield'\n",
    "if \"yield\" not in yield_df.columns and \"value\" in yield_df.columns:\n",
    "    yield_df = yield_df.rename(columns={\"value\": \"yield\"})\n",
    "\n",
    "# Process and merge all climate datasets\n",
    "merged_climate = None\n",
    "for prefix, path in climate_files.items():\n",
    "    climate_df = process_monthly_climate(path, prefix, lookup_gdf)\n",
    "    if merged_climate is None:\n",
    "        merged_climate = climate_df\n",
    "    else:\n",
    "        # Outer join to accumulate all monthly features\n",
    "        merged_climate = pd.merge(merged_climate, climate_df, on=[\"country\", \"year\"], how=\"outer\")\n",
    "\n",
    "land_cover_df['geometry'] = [Point(xy) for xy in zip(land_cover_df['longitude'], land_cover_df['latitude'])]\n",
    "land_cover_gdf = gpd.GeoDataFrame(land_cover_df, geometry='geometry', crs='EPSG:4326')\n",
    "# Join each land cover point to nearest country point\n",
    "land_cov_joined = gpd.sjoin_nearest(land_cover_gdf.to_crs(\"EPSG:3857\"), lookup_gdf.to_crs(\"EPSG:3857\"), how='left')\n",
    "land_cov_joined = land_cov_joined.to_crs(\"EPSG:4326\").drop(columns='geometry')\n",
    "# Rename land cover class columns to standardized names\n",
    "land_class_cols = [col for col in land_cover_df.columns if \"class_\" in col.lower()]\n",
    "land_cov_col_map = {col: f\"mean_cov_{col.split('_')[-1]}\" for col in land_class_cols}\n",
    "land_cov_joined = land_cov_joined.rename(columns=land_cov_col_map)\n",
    "# Aggregate land cover by country (mean percent per class)\n",
    "land_cov_summary = land_cov_joined.groupby(\"country\")[list(land_cov_col_map.values())].mean().reset_index()\n",
    "\n",
    "# Merge with climate on country + year\n",
    "climate_with_land = pd.merge(merged_climate, land_cov_summary, on=\"country\", how=\"left\")\n",
    "\n",
    "# Merge with yield data \n",
    "final_df = pd.merge(yield_df, climate_with_land, on=[\"country\", \"year\"], how=\"inner\")\n",
    "\n",
    "# # Organize column order\n",
    "# def ordered_columns(final_df):\n",
    "#     rain_cols = [f\"rain_{i}\" for i in range(1, 13)]\n",
    "#     snow_cols = [f\"snow_{i}\" for i in range(1, 13)]\n",
    "#     esoil_cols = [f\"esoil_{i}\" for i in range(1, 13)]\n",
    "#     soilm_cols, soilt_cols = [], []\n",
    "#     for layer in [\"0_10\", \"10_40\", \"40_100\", \"100_200\"]:\n",
    "#         soilm_cols += [f\"soilmoisture_{layer}_{i}\" for i in range(1, 13)]\n",
    "#         soilt_cols += [f\"soiltemp_{layer}_{i}\" for i in range(1, 13)]\n",
    "\n",
    "#     tveg_cols = [f\"tveg_{i}\" for i in range(1, 13)]\n",
    "#     tws_cols = [f\"tws_{i}\" for i in range(1, 13)]\n",
    "#     canopint_cols = [f\"canopint_{i}\" for i in range(1, 13)]\n",
    "#     landcov_cols = [f\"mean_cov_{i}\" for i in range(1, 18)]\n",
    "\n",
    "#     base_cols = [\"country\", \"item\", \"year\"]\n",
    "#     # Combine all expected features (filter only those that exist in the final DataFrame)\n",
    "#     all_features = landcov_cols + rain_cols + snow_cols + esoil_cols + soilm_cols + soilt_cols + tveg_cols + tws_cols + canopint_cols\n",
    "#     all_features = [col for col in all_features if col in final_df.columns]\n",
    "\n",
    "#     return base_cols + all_features + [\"yield\"]\n",
    "\n",
    "def ordered_columns(final_df):\n",
    "    base_cols = [\"country\", \"item\", \"year\"]\n",
    "    landcov_cols = [f\"mean_cov_{i}\" for i in range(1, 18)]\n",
    "\n",
    "    def monthly_block(prefix):\n",
    "        # 12 monthly columns\n",
    "        months = [f\"{prefix}_{i}\" for i in range(1, 13)]\n",
    "        # Corresponding summary columns\n",
    "        summaries = [f\"{prefix}_{stat}\" for stat in ['mean', 'std', 'min', 'max']]\n",
    "        # Include only those that exist in the DataFrame\n",
    "        return [col for col in months + summaries if col in final_df.columns]\n",
    "\n",
    "    feature_blocks = []\n",
    "\n",
    "    # Add monthly + summary blocks for each\n",
    "    prefixes = [\n",
    "        \"rain\", \"snow\", \"esoil\",\n",
    "        \"soilmoisture_0_10\", \"soilmoisture_10_40\", \"soilmoisture_40_100\", \"soilmoisture_100_200\",\n",
    "        \"soiltemp_0_10\", \"soiltemp_10_40\", \"soiltemp_40_100\", \"soiltemp_100_200\",\n",
    "        \"tveg\", \"tws\", \"canopint\"\n",
    "    ]\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        feature_blocks.extend(monthly_block(prefix))\n",
    "\n",
    "    # Final ordered list\n",
    "    all_features = landcov_cols + feature_blocks\n",
    "    all_features = [col for col in all_features if col in final_df.columns]\n",
    "\n",
    "    return base_cols + all_features + [\"yield\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adding summary\n",
    "# Function to add summary stats for 12-month feature groups\n",
    "def add_monthly_summary_features(df, feature_prefixes):\n",
    "    for prefix in feature_prefixes:\n",
    "        month_cols = [f\"{prefix}_{i}\" for i in range(1, 13) if f\"{prefix}_{i}\" in df.columns]\n",
    "        if len(month_cols) == 12:  # only compute if all 12 months are present\n",
    "            df[f\"{prefix}_mean\"] = df[month_cols].mean(axis=1)\n",
    "            df[f\"{prefix}_std\"] = df[month_cols].std(axis=1)\n",
    "            df[f\"{prefix}_min\"] = df[month_cols].min(axis=1)\n",
    "            df[f\"{prefix}_max\"] = df[month_cols].max(axis=1)\n",
    "    return df\n",
    "\n",
    "monthly_prefixes = [\n",
    "    \"rain\", \"snow\", \"esoil\",\n",
    "    \"soilmoisture_0_10\", \"soilmoisture_10_40\", \"soilmoisture_40_100\", \"soilmoisture_100_200\",\n",
    "    \"soiltemp_0_10\", \"soiltemp_10_40\", \"soiltemp_40_100\", \"soiltemp_100_200\",\n",
    "    \"tveg\", \"tws\", \"canopint\"\n",
    "]\n",
    "\n",
    "final_df = add_monthly_summary_features(final_df, monthly_prefixes)\n",
    "\n",
    "# # Update output column list\n",
    "# summary_cols = [col for col in final_df.columns if any(stat in col for stat in [\"_mean\", \"_std\", \"_min\", \"_max\"])]\n",
    "# final_df = final_df[ordered_columns(final_df) + summary_cols]\n",
    "\n",
    "final_df = final_df[ordered_columns(final_df)]\n",
    "\n",
    "# Save final output\n",
    "final_df.to_csv(\"Core_Dataset.csv\", index=False)\n",
    "print(\"Core dataset saved as Core_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (67963, 245)\n",
      "Columns: ['year', 'country', 'item', 'mean_cov_1', 'mean_cov_2', 'mean_cov_3', 'mean_cov_4', 'mean_cov_5', 'mean_cov_6', 'mean_cov_7', 'mean_cov_8', 'mean_cov_9', 'mean_cov_10', 'mean_cov_11', 'mean_cov_12', 'mean_cov_13', 'mean_cov_14', 'mean_cov_15', 'mean_cov_16', 'mean_cov_17', 'rain_1', 'rain_2', 'rain_3', 'rain_4', 'rain_5', 'rain_6', 'rain_7', 'rain_8', 'rain_9', 'rain_10', 'rain_11', 'rain_12', 'rain_mean', 'rain_std', 'rain_min', 'rain_max', 'snow_1', 'snow_2', 'snow_3', 'snow_4', 'snow_5', 'snow_6', 'snow_7', 'snow_8', 'snow_9', 'snow_10', 'snow_11', 'snow_12', 'snow_mean', 'snow_std', 'snow_min', 'snow_max', 'esoil_1', 'esoil_2', 'esoil_3', 'esoil_4', 'esoil_5', 'esoil_6', 'esoil_7', 'esoil_8', 'esoil_9', 'esoil_10', 'esoil_11', 'esoil_12', 'esoil_mean', 'esoil_std', 'esoil_min', 'esoil_max', 'soilmoisture_0_10_1', 'soilmoisture_0_10_2', 'soilmoisture_0_10_3', 'soilmoisture_0_10_4', 'soilmoisture_0_10_5', 'soilmoisture_0_10_6', 'soilmoisture_0_10_7', 'soilmoisture_0_10_8', 'soilmoisture_0_10_9', 'soilmoisture_0_10_10', 'soilmoisture_0_10_11', 'soilmoisture_0_10_12', 'soilmoisture_0_10_mean', 'soilmoisture_0_10_std', 'soilmoisture_0_10_min', 'soilmoisture_0_10_max', 'soilmoisture_10_40_1', 'soilmoisture_10_40_2', 'soilmoisture_10_40_3', 'soilmoisture_10_40_4', 'soilmoisture_10_40_5', 'soilmoisture_10_40_6', 'soilmoisture_10_40_7', 'soilmoisture_10_40_8', 'soilmoisture_10_40_9', 'soilmoisture_10_40_10', 'soilmoisture_10_40_11', 'soilmoisture_10_40_12', 'soilmoisture_10_40_mean', 'soilmoisture_10_40_std', 'soilmoisture_10_40_min', 'soilmoisture_10_40_max', 'soilmoisture_40_100_1', 'soilmoisture_40_100_2', 'soilmoisture_40_100_3', 'soilmoisture_40_100_4', 'soilmoisture_40_100_5', 'soilmoisture_40_100_6', 'soilmoisture_40_100_7', 'soilmoisture_40_100_8', 'soilmoisture_40_100_9', 'soilmoisture_40_100_10', 'soilmoisture_40_100_11', 'soilmoisture_40_100_12', 'soilmoisture_40_100_mean', 'soilmoisture_40_100_std', 'soilmoisture_40_100_min', 'soilmoisture_40_100_max', 'soilmoisture_100_200_1', 'soilmoisture_100_200_2', 'soilmoisture_100_200_3', 'soilmoisture_100_200_4', 'soilmoisture_100_200_5', 'soilmoisture_100_200_6', 'soilmoisture_100_200_7', 'soilmoisture_100_200_8', 'soilmoisture_100_200_9', 'soilmoisture_100_200_10', 'soilmoisture_100_200_11', 'soilmoisture_100_200_12', 'soilmoisture_100_200_mean', 'soilmoisture_100_200_std', 'soilmoisture_100_200_min', 'soilmoisture_100_200_max', 'soiltemp_0_10_1', 'soiltemp_0_10_2', 'soiltemp_0_10_3', 'soiltemp_0_10_4', 'soiltemp_0_10_5', 'soiltemp_0_10_6', 'soiltemp_0_10_7', 'soiltemp_0_10_8', 'soiltemp_0_10_9', 'soiltemp_0_10_10', 'soiltemp_0_10_11', 'soiltemp_0_10_12', 'soiltemp_0_10_mean', 'soiltemp_0_10_std', 'soiltemp_0_10_min', 'soiltemp_0_10_max', 'soiltemp_10_40_1', 'soiltemp_10_40_2', 'soiltemp_10_40_3', 'soiltemp_10_40_4', 'soiltemp_10_40_5', 'soiltemp_10_40_6', 'soiltemp_10_40_7', 'soiltemp_10_40_8', 'soiltemp_10_40_9', 'soiltemp_10_40_10', 'soiltemp_10_40_11', 'soiltemp_10_40_12', 'soiltemp_10_40_mean', 'soiltemp_10_40_std', 'soiltemp_10_40_min', 'soiltemp_10_40_max', 'soiltemp_40_100_1', 'soiltemp_40_100_2', 'soiltemp_40_100_3', 'soiltemp_40_100_4', 'soiltemp_40_100_5', 'soiltemp_40_100_6', 'soiltemp_40_100_7', 'soiltemp_40_100_8', 'soiltemp_40_100_9', 'soiltemp_40_100_10', 'soiltemp_40_100_11', 'soiltemp_40_100_12', 'soiltemp_40_100_mean', 'soiltemp_40_100_std', 'soiltemp_40_100_min', 'soiltemp_40_100_max', 'soiltemp_100_200_1', 'soiltemp_100_200_2', 'soiltemp_100_200_3', 'soiltemp_100_200_4', 'soiltemp_100_200_5', 'soiltemp_100_200_6', 'soiltemp_100_200_7', 'soiltemp_100_200_8', 'soiltemp_100_200_9', 'soiltemp_100_200_10', 'soiltemp_100_200_11', 'soiltemp_100_200_12', 'soiltemp_100_200_mean', 'soiltemp_100_200_std', 'soiltemp_100_200_min', 'soiltemp_100_200_max', 'tveg_1', 'tveg_2', 'tveg_3', 'tveg_4', 'tveg_5', 'tveg_6', 'tveg_7', 'tveg_8', 'tveg_9', 'tveg_10', 'tveg_11', 'tveg_12', 'tveg_mean', 'tveg_std', 'tveg_min', 'tveg_max', 'tws_1', 'tws_2', 'tws_3', 'tws_4', 'tws_5', 'tws_6', 'tws_7', 'tws_8', 'tws_9', 'tws_10', 'tws_11', 'tws_12', 'tws_mean', 'tws_std', 'tws_min', 'tws_max', 'canopint_1', 'canopint_2', 'canopint_3', 'canopint_4', 'canopint_5', 'canopint_6', 'canopint_7', 'canopint_8', 'canopint_9', 'canopint_10', 'canopint_11', 'canopint_12', 'canopint_mean', 'canopint_std', 'canopint_min', 'canopint_max', 'yield']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"Core_Dataset.csv\")\n",
    "\n",
    "# Move 'year' to the front\n",
    "cols = df.columns.tolist()\n",
    "if 'year' in cols:\n",
    "    cols.insert(0, cols.pop(cols.index('year')))\n",
    "df = df[cols]\n",
    "\n",
    "# Display structure\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years in dataset: [np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022)]\n"
     ]
    }
   ],
   "source": [
    "# Convert year to integer format (e.g. 2010, 2011, ...)\n",
    "df['year_raw'] = df['year'].astype(int)  # backup for clean export\n",
    "# Optional sanity check\n",
    "print(\"Years in dataset:\", sorted(df['year'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift yield by -1 to make next year's yield the prediction target\n",
    "df = df.sort_values(['country', 'item', 'year_raw'])  # ensure proper chronological order\n",
    "df['target'] = df.groupby(['country', 'item'])['yield'].shift(-1)\n",
    "\n",
    "# Drop rows where target is missing (i.e., last year of each country/item group)\n",
    "df = df.dropna(subset=['target']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_aware_split(df, test_year, val_year):\n",
    "    train_list, val_list, test_list = [], [], []\n",
    "\n",
    "    for _, group in df.groupby(['country', 'item']):\n",
    "        group = group.sort_values('year_raw')\n",
    "        train = group[group['year_raw'] < val_year]\n",
    "        val = group[group['year_raw'] == val_year]\n",
    "        test = group[group['year_raw'] == test_year]\n",
    "        train_list.append(train)\n",
    "        val_list.append(val)\n",
    "        test_list.append(test)\n",
    "\n",
    "    return pd.concat(train_list).reset_index(drop=True), \\\n",
    "           pd.concat(val_list).reset_index(drop=True), \\\n",
    "           pd.concat(test_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode country\n",
    "country_encoder = LabelEncoder()\n",
    "df['country_id'] = country_encoder.fit_transform(df['country'])\n",
    "num_countries = df['country_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode year as categorical ID for embedding\n",
    "year_encoder = LabelEncoder()\n",
    "df['year_id'] = year_encoder.fit_transform(df['year_raw'])  # not year+1, just original year\n",
    "num_years = df['year_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (52163, 249)\n",
      "Val: (5166, 249)\n",
      "Test: (5169, 249)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = time_aware_split(df, test_year=2021, val_year=2020)\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Val:\", val_df.shape)\n",
    "print(\"Test:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'item'\n",
    "df = pd.get_dummies(df, columns=['item'], prefix='item')\n",
    "item_onehot_cols = [col for col in df.columns if col.startswith('item_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sample:\n",
      "           country  country_id  year_raw  year_id\n",
      "0      Afghanistan           0      2010        0\n",
      "40411      Morocco         100      2010        0\n",
      "58993     Thailand         145      2010        0\n",
      "17395      Czechia          40      2010        0\n",
      "41204   Mozambique         101      2010        0\n",
      "Total countries: 165\n",
      "Total years: 12\n",
      "One-hot item columns: ['item_Abaca, manila hemp, raw', 'item_Agave fibres, raw, n.e.c.', 'item_Apples', 'item_Apricots', 'item_Avocados', 'item_Bambara beans, dry', 'item_Bananas', 'item_Barley', 'item_Beans, dry', 'item_Blueberries', 'item_Broad beans and horse beans, dry', 'item_Buckwheat', 'item_Canary seed', 'item_Cantaloupes and other melons', 'item_Cashewapple', 'item_Castor oil seeds', 'item_Cereals n.e.c.', 'item_Cherries', 'item_Chick peas, dry', 'item_Coconuts, in shell', 'item_Cow peas, dry', 'item_Cranberries', 'item_Currants', 'item_Dates', 'item_Figs', 'item_Flax, raw or retted', 'item_Fonio', 'item_Gooseberries', 'item_Grapes', 'item_Groundnuts, excluding shelled', 'item_Hempseed', 'item_Jojoba seeds', 'item_Jute, raw or retted', 'item_Kapok fruit', 'item_Karite nuts (sheanuts)', 'item_Kenaf, and other textile bast fibres, raw or retted', 'item_Kiwi fruit', 'item_Lemons and limes', 'item_Lentils, dry', 'item_Linseed', 'item_Locust beans (carobs)', 'item_Lupins', 'item_Maize (corn)', 'item_Mangoes, guavas and mangosteens', 'item_Melonseed', 'item_Millet', 'item_Mixed grain', 'item_Mustard seed', 'item_Oats', 'item_Oil palm fruit', 'item_Olives', 'item_Oranges', 'item_Other berries and fruits of the genus vaccinium n.e.c.', 'item_Other citrus fruit, n.e.c.', 'item_Other fibre crops, raw, n.e.c.', 'item_Other fruits, n.e.c.', 'item_Other oil seeds, n.e.c.', 'item_Other pome fruits', 'item_Other pulses n.e.c.', 'item_Other stone fruits', 'item_Other sugar crops n.e.c.', 'item_Other tropical fruits, n.e.c.', 'item_Papayas', 'item_Peaches and nectarines', 'item_Pears', 'item_Peas, dry', 'item_Persimmons', 'item_Pigeon peas, dry', 'item_Pineapples', 'item_Plantains and cooking bananas', 'item_Plums and sloes', 'item_Pomelos and grapefruits', 'item_Poppy seed', 'item_Quinces', 'item_Quinoa', 'item_Ramie, raw or retted', 'item_Rape or colza seed', 'item_Raspberries', 'item_Rice', 'item_Roots and Tubers, Total', 'item_Rye', 'item_Safflower seed', 'item_Seed cotton, unginned', 'item_Sesame seed', 'item_Sisal, raw', 'item_Sorghum', 'item_Sour cherries', 'item_Soya beans', 'item_Strawberries', 'item_Sugar beet', 'item_Sugar cane', 'item_Sunflower seed', 'item_Tallowtree seeds', 'item_Tangerines, mandarins, clementines', 'item_Treenuts, Total', 'item_Triticale', 'item_True hemp, raw or retted', 'item_Tung nuts', 'item_Vegetables Primary', 'item_Vetches', 'item_Watermelons', 'item_Wheat']\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded Sample:\")\n",
    "print(df[['country', 'country_id', 'year_raw', 'year_id']].drop_duplicates().sort_values('year_raw').head())\n",
    "\n",
    "print(f\"Total countries: {num_countries}\")\n",
    "print(f\"Total years: {num_years}\")\n",
    "print(f\"One-hot item columns: {item_onehot_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_onehot_cols = [col for col in df.columns if col.startswith('item_')]\n",
    "\n",
    "# Re-add any missing one-hot columns in val/test with zeros\n",
    "for col in item_onehot_cols:\n",
    "    for dset in [df]:\n",
    "        if col not in dset.columns:\n",
    "            dset[col] = 0\n",
    "\n",
    "# Columns to exclude from scaling\n",
    "exclude = ['yield', 'target', 'country', 'country_id', 'year_raw', 'year_id']\n",
    "exclude += item_onehot_cols  # exclude one-hot item columns too\n",
    "\n",
    "# Features to scale (everything else)\n",
    "feature_cols = [col for col in df.columns if col not in exclude]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit only on train\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "\n",
    "# Transform val and test\n",
    "val_df[feature_cols] = scaler.transform(val_df[feature_cols])\n",
    "test_df[feature_cols] = scaler.transform(test_df[feature_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the Target Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/pdkq_2cj66gd5py_6r6srh8r0000gn/T/ipykernel_53748/4136113568.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['target_scaled'] = target_scaler.fit_transform(train_df[['target']])\n",
      "/var/folders/fw/pdkq_2cj66gd5py_6r6srh8r0000gn/T/ipykernel_53748/4136113568.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_df['target_scaled'] = target_scaler.transform(val_df[['target']])\n",
      "/var/folders/fw/pdkq_2cj66gd5py_6r6srh8r0000gn/T/ipykernel_53748/4136113568.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df['target_scaled'] = target_scaler.transform(test_df[['target']])\n"
     ]
    }
   ],
   "source": [
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "train_df['target_scaled'] = target_scaler.fit_transform(train_df[['target']])\n",
    "val_df['target_scaled'] = target_scaler.transform(val_df[['target']])\n",
    "test_df['target_scaled'] = target_scaler.transform(test_df[['target']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fill any NaNs (if any exist after merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].mean())\n",
    "val_df[feature_cols] = val_df[feature_cols].fillna(train_df[feature_cols].mean())  # use train stats\n",
    "test_df[feature_cols] = test_df[feature_cols].fillna(train_df[feature_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature sample after scaling:\n",
      "       year  mean_cov_1  mean_cov_2  mean_cov_3  mean_cov_4  mean_cov_5  \\\n",
      "0  0.000000         1.0         0.0         0.0         0.0         0.0   \n",
      "1  0.111111         1.0         0.0         0.0         0.0         0.0   \n",
      "2  0.222222         1.0         0.0         0.0         0.0         0.0   \n",
      "3  0.333333         1.0         0.0         0.0         0.0         0.0   \n",
      "4  0.444444         1.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "   mean_cov_6  mean_cov_7  mean_cov_8  mean_cov_9  ...  canopint_7  \\\n",
      "0         0.0         0.0         0.0         0.0  ...    0.013346   \n",
      "1         0.0         0.0         0.0         0.0  ...    0.014067   \n",
      "2         0.0         0.0         0.0         0.0  ...    0.015859   \n",
      "3         0.0         0.0         0.0         0.0  ...    0.010501   \n",
      "4         0.0         0.0         0.0         0.0  ...    0.015142   \n",
      "\n",
      "   canopint_8  canopint_9  canopint_10  canopint_11  canopint_12  \\\n",
      "0    0.023366    0.018008     0.008390     0.007517     0.005843   \n",
      "1    0.017798    0.025853     0.022088     0.024257     0.008902   \n",
      "2    0.021423    0.026218     0.010130     0.010611     0.026112   \n",
      "3    0.019933    0.018395     0.022105     0.012700     0.013604   \n",
      "4    0.018696    0.028011     0.027487     0.019772     0.010866   \n",
      "\n",
      "   canopint_mean  canopint_std  canopint_min  canopint_max  \n",
      "0       0.019412      0.020823      0.008891      0.025962  \n",
      "1       0.024643      0.025536      0.013526      0.035569  \n",
      "2       0.026648      0.026985      0.014226      0.034911  \n",
      "3       0.023959      0.023268      0.016629      0.029113  \n",
      "4       0.026791      0.021149      0.016502      0.029099  \n",
      "\n",
      "[5 rows x 242 columns]\n",
      "Target range (train): 0.0 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature sample after scaling:\")\n",
    "print(train_df[feature_cols].head())\n",
    "print(\"Target range (train):\", train_df['target_scaled'].min(), train_df['target_scaled'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols += item_onehot_cols  # include one-hot columns last (will not scale them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. One-hot encode 'item' again, now in each split\n",
    "train_df = pd.get_dummies(train_df, columns=['item'], prefix='item')\n",
    "val_df = pd.get_dummies(val_df, columns=['item'], prefix='item')\n",
    "test_df = pd.get_dummies(test_df, columns=['item'], prefix='item')\n",
    "\n",
    "# 2. Ensure all splits have the same item_* columns\n",
    "all_item_cols = sorted(set(train_df.columns) | set(val_df.columns) | set(test_df.columns))\n",
    "item_onehot_cols = [col for col in all_item_cols if col.startswith(\"item_\")]\n",
    "\n",
    "for col in item_onehot_cols:\n",
    "    for dset in [train_df, val_df, test_df]:\n",
    "        if col not in dset.columns:\n",
    "            dset[col] = 0\n",
    "\n",
    "# 3. Reorder to maintain consistent column structure\n",
    "train_df = train_df.reindex(columns=sorted(train_df.columns))\n",
    "val_df = val_df.reindex(columns=sorted(val_df.columns))\n",
    "test_df = test_df.reindex(columns=sorted(test_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature columns: ['canopint_1', 'canopint_10', 'canopint_11', 'canopint_12', 'canopint_2', 'canopint_3', 'canopint_4', 'canopint_5', 'canopint_6', 'canopint_7', 'canopint_8', 'canopint_9', 'canopint_max', 'canopint_mean', 'canopint_min', 'canopint_std', 'esoil_1', 'esoil_10', 'esoil_11', 'esoil_12', 'esoil_2', 'esoil_3', 'esoil_4', 'esoil_5', 'esoil_6', 'esoil_7', 'esoil_8', 'esoil_9', 'esoil_max', 'esoil_mean', 'esoil_min', 'esoil_std', 'mean_cov_1', 'mean_cov_10', 'mean_cov_11', 'mean_cov_12', 'mean_cov_13', 'mean_cov_14', 'mean_cov_15', 'mean_cov_16', 'mean_cov_17', 'mean_cov_2', 'mean_cov_3', 'mean_cov_4', 'mean_cov_5', 'mean_cov_6', 'mean_cov_7', 'mean_cov_8', 'mean_cov_9', 'rain_1', 'rain_10', 'rain_11', 'rain_12', 'rain_2', 'rain_3', 'rain_4', 'rain_5', 'rain_6', 'rain_7', 'rain_8', 'rain_9', 'rain_max', 'rain_mean', 'rain_min', 'rain_std', 'snow_1', 'snow_10', 'snow_11', 'snow_12', 'snow_2', 'snow_3', 'snow_4', 'snow_5', 'snow_6', 'snow_7', 'snow_8', 'snow_9', 'snow_max', 'snow_mean', 'snow_min', 'snow_std', 'soilmoisture_0_10_1', 'soilmoisture_0_10_10', 'soilmoisture_0_10_11', 'soilmoisture_0_10_12', 'soilmoisture_0_10_2', 'soilmoisture_0_10_3', 'soilmoisture_0_10_4', 'soilmoisture_0_10_5', 'soilmoisture_0_10_6', 'soilmoisture_0_10_7', 'soilmoisture_0_10_8', 'soilmoisture_0_10_9', 'soilmoisture_0_10_max', 'soilmoisture_0_10_mean', 'soilmoisture_0_10_min', 'soilmoisture_0_10_std', 'soilmoisture_100_200_1', 'soilmoisture_100_200_10', 'soilmoisture_100_200_11', 'soilmoisture_100_200_12', 'soilmoisture_100_200_2', 'soilmoisture_100_200_3', 'soilmoisture_100_200_4', 'soilmoisture_100_200_5', 'soilmoisture_100_200_6', 'soilmoisture_100_200_7', 'soilmoisture_100_200_8', 'soilmoisture_100_200_9', 'soilmoisture_100_200_max', 'soilmoisture_100_200_mean', 'soilmoisture_100_200_min', 'soilmoisture_100_200_std', 'soilmoisture_10_40_1', 'soilmoisture_10_40_10', 'soilmoisture_10_40_11', 'soilmoisture_10_40_12', 'soilmoisture_10_40_2', 'soilmoisture_10_40_3', 'soilmoisture_10_40_4', 'soilmoisture_10_40_5', 'soilmoisture_10_40_6', 'soilmoisture_10_40_7', 'soilmoisture_10_40_8', 'soilmoisture_10_40_9', 'soilmoisture_10_40_max', 'soilmoisture_10_40_mean', 'soilmoisture_10_40_min', 'soilmoisture_10_40_std', 'soilmoisture_40_100_1', 'soilmoisture_40_100_10', 'soilmoisture_40_100_11', 'soilmoisture_40_100_12', 'soilmoisture_40_100_2', 'soilmoisture_40_100_3', 'soilmoisture_40_100_4', 'soilmoisture_40_100_5', 'soilmoisture_40_100_6', 'soilmoisture_40_100_7', 'soilmoisture_40_100_8', 'soilmoisture_40_100_9', 'soilmoisture_40_100_max', 'soilmoisture_40_100_mean', 'soilmoisture_40_100_min', 'soilmoisture_40_100_std', 'soiltemp_0_10_1', 'soiltemp_0_10_10', 'soiltemp_0_10_11', 'soiltemp_0_10_12', 'soiltemp_0_10_2', 'soiltemp_0_10_3', 'soiltemp_0_10_4', 'soiltemp_0_10_5', 'soiltemp_0_10_6', 'soiltemp_0_10_7', 'soiltemp_0_10_8', 'soiltemp_0_10_9', 'soiltemp_0_10_max', 'soiltemp_0_10_mean', 'soiltemp_0_10_min', 'soiltemp_0_10_std', 'soiltemp_100_200_1', 'soiltemp_100_200_10', 'soiltemp_100_200_11', 'soiltemp_100_200_12', 'soiltemp_100_200_2', 'soiltemp_100_200_3', 'soiltemp_100_200_4', 'soiltemp_100_200_5', 'soiltemp_100_200_6', 'soiltemp_100_200_7', 'soiltemp_100_200_8', 'soiltemp_100_200_9', 'soiltemp_100_200_max', 'soiltemp_100_200_mean', 'soiltemp_100_200_min', 'soiltemp_100_200_std', 'soiltemp_10_40_1', 'soiltemp_10_40_10', 'soiltemp_10_40_11', 'soiltemp_10_40_12', 'soiltemp_10_40_2', 'soiltemp_10_40_3', 'soiltemp_10_40_4', 'soiltemp_10_40_5', 'soiltemp_10_40_6', 'soiltemp_10_40_7', 'soiltemp_10_40_8', 'soiltemp_10_40_9', 'soiltemp_10_40_max', 'soiltemp_10_40_mean', 'soiltemp_10_40_min', 'soiltemp_10_40_std', 'soiltemp_40_100_1', 'soiltemp_40_100_10', 'soiltemp_40_100_11', 'soiltemp_40_100_12', 'soiltemp_40_100_2', 'soiltemp_40_100_3', 'soiltemp_40_100_4', 'soiltemp_40_100_5', 'soiltemp_40_100_6', 'soiltemp_40_100_7', 'soiltemp_40_100_8', 'soiltemp_40_100_9', 'soiltemp_40_100_max', 'soiltemp_40_100_mean', 'soiltemp_40_100_min', 'soiltemp_40_100_std', 'target_scaled', 'tveg_1', 'tveg_10', 'tveg_11', 'tveg_12', 'tveg_2', 'tveg_3', 'tveg_4', 'tveg_5', 'tveg_6', 'tveg_7', 'tveg_8', 'tveg_9', 'tveg_max', 'tveg_mean', 'tveg_min', 'tveg_std', 'tws_1', 'tws_10', 'tws_11', 'tws_12', 'tws_2', 'tws_3', 'tws_4', 'tws_5', 'tws_6', 'tws_7', 'tws_8', 'tws_9', 'tws_max', 'tws_mean', 'tws_min', 'tws_std', 'item_Abaca, manila hemp, raw', 'item_Agave fibres, raw, n.e.c.', 'item_Apples', 'item_Apricots', 'item_Avocados', 'item_Bambara beans, dry', 'item_Bananas', 'item_Barley', 'item_Beans, dry', 'item_Blueberries', 'item_Broad beans and horse beans, dry', 'item_Buckwheat', 'item_Canary seed', 'item_Cantaloupes and other melons', 'item_Cashewapple', 'item_Castor oil seeds', 'item_Cereals n.e.c.', 'item_Cherries', 'item_Chick peas, dry', 'item_Coconuts, in shell', 'item_Cow peas, dry', 'item_Cranberries', 'item_Currants', 'item_Dates', 'item_Figs', 'item_Flax, raw or retted', 'item_Fonio', 'item_Gooseberries', 'item_Grapes', 'item_Groundnuts, excluding shelled', 'item_Hempseed', 'item_Jojoba seeds', 'item_Jute, raw or retted', 'item_Kapok fruit', 'item_Karite nuts (sheanuts)', 'item_Kenaf, and other textile bast fibres, raw or retted', 'item_Kiwi fruit', 'item_Lemons and limes', 'item_Lentils, dry', 'item_Linseed', 'item_Locust beans (carobs)', 'item_Lupins', 'item_Maize (corn)', 'item_Mangoes, guavas and mangosteens', 'item_Melonseed', 'item_Millet', 'item_Mixed grain', 'item_Mustard seed', 'item_Oats', 'item_Oil palm fruit', 'item_Olives', 'item_Oranges', 'item_Other berries and fruits of the genus vaccinium n.e.c.', 'item_Other citrus fruit, n.e.c.', 'item_Other fibre crops, raw, n.e.c.', 'item_Other fruits, n.e.c.', 'item_Other oil seeds, n.e.c.', 'item_Other pome fruits', 'item_Other pulses n.e.c.', 'item_Other stone fruits', 'item_Other sugar crops n.e.c.', 'item_Other tropical fruits, n.e.c.', 'item_Papayas', 'item_Peaches and nectarines', 'item_Pears', 'item_Peas, dry', 'item_Persimmons', 'item_Pigeon peas, dry', 'item_Pineapples', 'item_Plantains and cooking bananas', 'item_Plums and sloes', 'item_Pomelos and grapefruits', 'item_Poppy seed', 'item_Quinces', 'item_Quinoa', 'item_Ramie, raw or retted', 'item_Rape or colza seed', 'item_Raspberries', 'item_Rice', 'item_Roots and Tubers, Total', 'item_Rye', 'item_Safflower seed', 'item_Seed cotton, unginned', 'item_Sesame seed', 'item_Sisal, raw', 'item_Sorghum', 'item_Sour cherries', 'item_Soya beans', 'item_Strawberries', 'item_Sugar beet', 'item_Sugar cane', 'item_Sunflower seed', 'item_Tallowtree seeds', 'item_Tangerines, mandarins, clementines', 'item_Treenuts, Total', 'item_Triticale', 'item_True hemp, raw or retted', 'item_Tung nuts', 'item_Vegetables Primary', 'item_Vetches', 'item_Watermelons', 'item_Wheat']\n"
     ]
    }
   ],
   "source": [
    "# Recreate the list of features\n",
    "exclude = ['yield', 'target', 'country', 'year', 'year_raw', 'country_id', 'year_id']\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude and not col.startswith('item_')]\n",
    "feature_cols += item_onehot_cols  # now safe to add\n",
    "\n",
    "print(\"Final feature columns:\", feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all features are numeric floats\n",
    "train_df[feature_cols] = train_df[feature_cols].apply(pd.to_numeric, errors='coerce').astype('float32')\n",
    "val_df[feature_cols] = val_df[feature_cols].apply(pd.to_numeric, errors='coerce').astype('float32')\n",
    "test_df[feature_cols] = test_df[feature_cols].apply(pd.to_numeric, errors='coerce').astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FALSE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m USE_SUMMARY_FEATURES \u001b[38;5;241m=\u001b[39m \u001b[43mFALSE\u001b[49m  \u001b[38;5;66;03m# set to False to disable summary stats\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FALSE' is not defined"
     ]
    }
   ],
   "source": [
    "USE_SUMMARY_FEATURES = False  # set to False to disable summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify one-hot item columns again (after splitting)\n",
    "item_onehot_cols = [col for col in train_df.columns if col.startswith(\"item_\")]\n",
    "\n",
    "# Identify all summary columns (mean/std/min/max)\n",
    "summary_cols = [col for col in train_df.columns if any(stat in col for stat in ['_mean', '_std', '_min', '_max'])]\n",
    "\n",
    "# Core features (excluding target/ids/etc)\n",
    "exclude = ['yield', 'target', 'country', 'item', 'year_raw', 'year', 'country_id', 'year_id']\n",
    "base_feature_cols = [col for col in train_df.columns if col not in exclude and not col.startswith(\"item_\") and not any(s in col for s in ['_mean', '_std', '_min', '_max'])]\n",
    "\n",
    "# Final feature set\n",
    "feature_cols = base_feature_cols + item_onehot_cols\n",
    "if USE_SUMMARY_FEATURES:\n",
    "    feature_cols += summary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"🔁 Using summary features: {USE_SUMMARY_FEATURES}\")\n",
    "print(f\"Total input features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train_num = torch.tensor(train_df[feature_cols].values, dtype=torch.float32)\n",
    "X_train_country = torch.tensor(train_df['country_id'].values, dtype=torch.long)\n",
    "X_train_year = torch.tensor(train_df['year_id'].values, dtype=torch.long)\n",
    "y_train = torch.tensor(train_df['target_scaled'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_val_num = torch.tensor(val_df[feature_cols].values, dtype=torch.float32)\n",
    "X_val_country = torch.tensor(val_df['country_id'].values, dtype=torch.long)\n",
    "X_val_year = torch.tensor(val_df['year_id'].values, dtype=torch.long)\n",
    "y_val = torch.tensor(val_df['target_scaled'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test_num = torch.tensor(test_df[feature_cols].values, dtype=torch.float32)\n",
    "X_test_country = torch.tensor(test_df['country_id'].values, dtype=torch.long)\n",
    "X_test_year = torch.tensor(test_df['year_id'].values, dtype=torch.long)\n",
    "y_test = torch.tensor(test_df['target_scaled'].values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Convert Train/Val/Test into PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # --- TRAIN tensors ---\n",
    "# X_train_num = torch.tensor(train_df[feature_cols].values, dtype=torch.float32)  # all numeric + one-hot\n",
    "# X_train_country = torch.tensor(train_df['country_id'].values, dtype=torch.long)  # for embedding\n",
    "# X_train_year = torch.tensor(train_df['year_id'].values, dtype=torch.long)        # for embedding\n",
    "# y_train = torch.tensor(train_df['target_scaled'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# # --- VALIDATION tensors ---\n",
    "# X_val_num = torch.tensor(val_df[feature_cols].values, dtype=torch.float32)\n",
    "# X_val_country = torch.tensor(val_df['country_id'].values, dtype=torch.long)\n",
    "# X_val_year = torch.tensor(val_df['year_id'].values, dtype=torch.long)\n",
    "# y_val = torch.tensor(val_df['target_scaled'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# # --- TEST tensors ---\n",
    "# X_test_num = torch.tensor(test_df[feature_cols].values, dtype=torch.float32)\n",
    "# X_test_country = torch.tensor(test_df['country_id'].values, dtype=torch.long)\n",
    "# X_test_year = torch.tensor(test_df['year_id'].values, dtype=torch.long)\n",
    "# y_test = torch.tensor(test_df['target_scaled'].values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_num, X_train_country, X_train_year, y_train),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "class MLPWithEmbeddings(nn.Module):\n",
    "    def __init__(self, input_dim, num_countries, num_years, emb_dim_country=8, emb_dim_year=4):\n",
    "        super().__init__()\n",
    "        self.country_emb = nn.Embedding(num_countries, emb_dim_country)\n",
    "        self.year_emb = nn.Embedding(num_years, emb_dim_year)\n",
    "\n",
    "        total_input_dim = input_dim + emb_dim_country + emb_dim_year\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # nn.Linear(total_input_dim, 256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.3),\n",
    "            # nn.Linear(256, 128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.3),\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            # nn.Linear(64, 1),\n",
    "            # nn.Sigmoid()  # Ensures output is between 0 and 1\n",
    "            nn.Linear(total_input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Clamp output\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_country, x_year):\n",
    "        c_vec = self.country_emb(x_country)\n",
    "        y_vec = self.year_emb(x_year)\n",
    "        x = torch.cat([x_num, c_vec, y_vec], dim=1)\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "model = MLPWithEmbeddings(\n",
    "    input_dim=X_train_num.shape[1],\n",
    "    num_countries=num_countries,\n",
    "    num_years=num_years,\n",
    "    emb_dim_country=8,\n",
    "    emb_dim_year=4\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "# --- Training with Validation ---\n",
    "train_losses, train_maes, val_losses, val_maes = [], [], [], []\n",
    "\n",
    "# --- Early stopping setup ---\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss, total_train_mae = 0.0, 0.0\n",
    "    for xb_num, xb_country, xb_year, yb in train_loader:\n",
    "        pred = model(xb_num, xb_country, xb_year)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * xb_num.size(0)\n",
    "        total_train_mae += mean_absolute_error(yb.detach().numpy(), pred.detach().numpy()) * xb_num.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    avg_train_mae = total_train_mae / len(train_loader.dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_maes.append(avg_train_mae)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss, total_val_mae = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_country, xb_year, yb in DataLoader(\n",
    "            TensorDataset(X_val_num, X_val_country, X_val_year, y_val),\n",
    "            batch_size=32,\n",
    "            shuffle=False\n",
    "        ):\n",
    "            pred = model(xb_num, xb_country, xb_year)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            total_val_loss += loss.item() * xb_num.size(0)\n",
    "            total_val_mae += mean_absolute_error(yb.numpy(), pred.numpy()) * xb_num.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(y_val)\n",
    "    avg_val_mae = total_val_mae / len(y_val)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_maes.append(avg_val_mae)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Train MAE: {avg_train_mae:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val MAE: {avg_val_mae:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        best_model_state = model.state_dict()  # Save best model so far\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"  🔁 No improvement for {counter} epoch(s).\")\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"\\n⏹️ Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "# --- Restore best model ---\n",
    "model.load_state_dict(best_model_state)\n",
    "print(\"✅ Best model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With parameter sweep to see optimal values of country and year embeddings (current: 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# import numpy as np\n",
    "\n",
    "# # Datasets and constants assumed to be defined:\n",
    "# # X_train_num, X_train_country, X_train_year, y_train\n",
    "# # X_val_num, X_val_country, X_val_year, y_val\n",
    "# # num_countries, num_years\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     TensorDataset(X_train_num, X_train_country, X_train_year, y_train),\n",
    "#     batch_size=32,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     TensorDataset(X_val_num, X_val_country, X_val_year, y_val),\n",
    "#     batch_size=32,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# class MLPWithEmbeddings(nn.Module):\n",
    "#     def __init__(self, input_dim, num_countries, num_years, emb_dim_country, emb_dim_year):\n",
    "#         super().__init__()\n",
    "#         self.country_emb = nn.Embedding(num_countries, emb_dim_country)\n",
    "#         self.year_emb = nn.Embedding(num_years, emb_dim_year)\n",
    "\n",
    "#         total_input_dim = input_dim + emb_dim_country + emb_dim_year\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(total_input_dim, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_num, x_country, x_year):\n",
    "#         c_vec = self.country_emb(x_country)\n",
    "#         y_vec = self.year_emb(x_year)\n",
    "#         x = torch.cat([x_num, c_vec, y_vec], dim=1)\n",
    "#         return self.model(x)\n",
    "\n",
    "# # Define sweep values\n",
    "# country_dims = [8, 12, 16, 32]\n",
    "# year_dims = [4, 6]\n",
    "# results = []\n",
    "\n",
    "# for emb_dim_country in country_dims:\n",
    "#     for emb_dim_year in year_dims:\n",
    "#         print(f\"\\n🔍 Training with emb_dim_country={emb_dim_country}, emb_dim_year={emb_dim_year}\")\n",
    "\n",
    "#         model = MLPWithEmbeddings(\n",
    "#             input_dim=X_train_num.shape[1],\n",
    "#             num_countries=num_countries,\n",
    "#             num_years=num_years,\n",
    "#             emb_dim_country=emb_dim_country,\n",
    "#             emb_dim_year=emb_dim_year\n",
    "#         )\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "#         loss_fn = nn.MSELoss()\n",
    "\n",
    "#         # Early stopping\n",
    "#         best_loss = float('inf')\n",
    "#         patience, counter = 5, 0\n",
    "#         best_model_state = None\n",
    "\n",
    "#         for epoch in range(100):\n",
    "#             model.train()\n",
    "#             total_train_loss, total_train_mae = 0.0, 0.0\n",
    "\n",
    "#             for xb_num, xb_country, xb_year, yb in train_loader:\n",
    "#                 pred = model(xb_num, xb_country, xb_year)\n",
    "#                 loss = loss_fn(pred, yb)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 total_train_loss += loss.item() * xb_num.size(0)\n",
    "#                 total_train_mae += mean_absolute_error(\n",
    "#                     yb.detach().numpy(), pred.detach().numpy()) * xb_num.size(0)\n",
    "\n",
    "#             avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "#             avg_train_mae = total_train_mae / len(train_loader.dataset)\n",
    "\n",
    "#             # Validation phase\n",
    "#             model.eval()\n",
    "#             total_val_loss, total_val_mae = 0.0, 0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for xb_num, xb_country, xb_year, yb in val_loader:\n",
    "#                     pred = model(xb_num, xb_country, xb_year)\n",
    "#                     loss = loss_fn(pred, yb)\n",
    "#                     total_val_loss += loss.item() * xb_num.size(0)\n",
    "#                     total_val_mae += mean_absolute_error(\n",
    "#                         yb.numpy(), pred.numpy()) * xb_num.size(0)\n",
    "\n",
    "#             avg_val_loss = total_val_loss / len(y_val)\n",
    "#             avg_val_mae = total_val_mae / len(y_val)\n",
    "\n",
    "#             print(f\"Epoch {epoch+1} - Train MAE: {avg_train_mae:.4f} | Val MAE: {avg_val_mae:.4f}\")\n",
    "\n",
    "#             if avg_val_loss < best_loss:\n",
    "#                 best_loss = avg_val_loss\n",
    "#                 best_mae = avg_val_mae\n",
    "#                 best_model_state = model.state_dict()\n",
    "#                 counter = 0\n",
    "#             else:\n",
    "#                 counter += 1\n",
    "#                 if counter >= patience:\n",
    "#                     print(\"⏹️ Early stopping\")\n",
    "#                     break\n",
    "\n",
    "#         results.append({\n",
    "#             \"emb_dim_country\": emb_dim_country,\n",
    "#             \"emb_dim_year\": emb_dim_year,\n",
    "#             \"val_mae\": best_mae\n",
    "#         })\n",
    "\n",
    "# # Display best configuration\n",
    "# best_config = sorted(results, key=lambda x: x[\"val_mae\"])[0]\n",
    "# print(\"\\n✅ Best Embedding Config:\")\n",
    "# print(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='o')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test_num, X_test_country, X_test_year).numpy().flatten()  # predicted (scaled)\n",
    "    y_true_test = y_test.numpy().flatten()                                          # true (scaled)\n",
    "\n",
    "# Inverse transform to original yield scale\n",
    "y_pred_rescaled = target_scaler.inverse_transform(y_pred_test.reshape(-1, 1)).flatten()\n",
    "y_true_rescaled = target_scaler.inverse_transform(y_true_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_true_rescaled, y_pred_rescaled)\n",
    "print(f\"\\n🧪 Test MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# Calculate Test Loss and Test MAE\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Predict on the test set\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test_num, X_test_country, X_test_year).numpy().flatten()  # predicted (scaled)\n",
    "    y_true_test = y_test.numpy().flatten()                                          # true (scaled)\n",
    "\n",
    "# Calculate Test Loss (MSE)\n",
    "test_loss = mean_squared_error(y_true_test, y_pred_test)\n",
    "\n",
    "# Calculate Test MAE\n",
    "test_mae = mean_absolute_error(y_true_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Prediction range:\", y_pred_test.min(), \"to\", y_pred_test.max())\n",
    "print(\"✅ True yield range:\", y_true_test.min(), \"to\", y_true_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true_test, y_pred_test, alpha=0.6)\n",
    "plt.plot([y_true_test.min(), y_true_test.max()],\n",
    "         [y_true_test.min(), y_true_test.max()],\n",
    "         'r--', label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Yield\")\n",
    "plt.ylabel(\"Predicted Yield\")\n",
    "plt.title(\"Predicted vs Actual Yield (Test Set)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Recover the item name from one-hot columns\n",
    "item_cols = [col for col in test_df.columns if col.startswith('item_')]\n",
    "test_df['item'] = test_df[item_cols].idxmax(axis=1).str.replace('item_', '')\n",
    "\n",
    "# 2. Restore predictions and ground truth\n",
    "test_df['yield_true'] = y_true_rescaled\n",
    "test_df['yield_pred'] = y_pred_rescaled\n",
    "\n",
    "# 3. Select clean columns for display\n",
    "results_table = test_df[['year_raw', 'country', 'item', 'yield_true', 'yield_pred']]\n",
    "results_table = results_table.rename(columns={'year_raw': 'year'})\n",
    "\n",
    "# 4. Preview the output\n",
    "print(results_table.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.to_csv(\"yield_predictions.csv\", index=False)\n",
    "print(\"✅ Results saved to 'yield_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true_rescaled, y_pred_rescaled)\n",
    "plt.plot([y_true_rescaled.min(), y_true_rescaled.max()],\n",
    "         [y_true_rescaled.min(), y_true_rescaled.max()],\n",
    "         'r--', label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Yield\")\n",
    "plt.ylabel(\"Predicted Yield\")\n",
    "plt.title(\"Predicted vs Actual Yield (Test Set)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['yield_true'] = y_true_rescaled\n",
    "test_df['yield_pred'] = y_pred_rescaled\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "year_mae = test_df.groupby('year_raw').apply(\n",
    "    lambda x: mean_absolute_error(x['yield_true'], x['yield_pred'])\n",
    ").reset_index(name='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(year_mae['year_raw'], year_mae['mae'], marker='o', linestyle='-')\n",
    "plt.title(\"Test MAE by Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean Absolute Error (Yield)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts = test_df['year_raw'].value_counts().sort_index()\n",
    "year_mae['count'] = year_counts.values\n",
    "print(year_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "errors = np.abs(y_pred_rescaled.flatten() - y_true_rescaled.flatten())\n",
    "top_outliers_idx = np.argsort(errors)[-2:]  # indices of top 2 worst predictions\n",
    "\n",
    "# Print the values for inspection\n",
    "for idx in top_outliers_idx:\n",
    "    print(f\"[{idx}] Actual: {y_true_rescaled[idx]:,.0f}, Predicted: {y_pred_rescaled[idx]:,.0f}, Error: {errors[idx]:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors = pd.DataFrame({\n",
    "    \"actual\": y_true_rescaled,\n",
    "    \"predicted\": y_pred_rescaled,\n",
    "    \"error\": errors,\n",
    "    \"country\": test_df[\"country\"].values,\n",
    "    \"item\": test_df[\"item\"].values,\n",
    "    \"year\": test_df[\"year_raw\"].values\n",
    "})\n",
    "print(df_errors.sort_values(\"error\", ascending=False).head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
