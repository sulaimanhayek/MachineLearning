{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcgYtozJ1r16"
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KnN4Z8d1xdQ"
   },
   "source": [
    "\n",
    "* To introduce you to PyTorch (https://pytorch.org/docs/stable/index.html), which is one of the most widely used software libraries for neural networks.\n",
    ">**Remember**: It is your responsibility as a machine learning scientist to read documentations for any library function you use and to thoroughly understand what it is doing, if it validly serves your purpose, and which of its parameters you need to consider.\n",
    "\n",
    "* To apply the perceptron and multilayer perceptron from Week 4 lecture to automatic detection of the number of days of ground frost and snow based on other weather variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mgn54mnC2C2F"
   },
   "source": [
    "# Section 1 - Load the UK Met (60km, 2010-2022) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVH-kE3j2JEC"
   },
   "source": [
    "Same as for the Week 3 lab,\n",
    "\n",
    "1. You need to first download the data before you can get started. Download from the Week 3 page for the module, on Canvas (see 'Week 3 Lab Dataset' on the page). The file you download will be named *curated_data_1month_2010-2022_nonans.csv*.\n",
    "\n",
    "2. Then, use the file menu in Google Colab to upload the file to your Colab directory. Once upload is complete, you should be able to see the file on the listed contents of your Colab directory.\n",
    "\n",
    "3. You can now run the code in the cell below to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mUakXgG72Amq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /content: No such file or directory\n",
      "\n",
      " The dataset has shape: (10296, 13)\n",
      "\n",
      " A peek at the dataset features: \n",
      "[[8.93698275e+01 1.02266536e+03 6.45115642e+01 ... 6.45810733e+00\n",
      "  6.72744772e+00 6.97199793e-01]\n",
      " [8.94462109e+01 1.02270800e+03 5.74868117e+01 ... 5.88191052e+00\n",
      "  6.23064828e+00 1.62952568e+00]\n",
      " [8.93435447e+01 1.02243684e+03 6.82935149e+01 ... 4.62830127e+00\n",
      "  6.29080656e+00 1.17293773e+00]\n",
      " ...\n",
      " [8.78370293e+01 1.00645706e+03 1.38800195e+01 ... 4.96064026e+00\n",
      "  1.85626301e+00 7.99709543e+00]\n",
      " [8.88116315e+01 1.00662248e+03 2.05853162e+01 ... 4.93635497e+00\n",
      "  7.75835354e-01 8.46815900e+00]\n",
      " [8.27601516e+01 1.00593830e+03 1.05309193e+01 ... 8.38081942e+00\n",
      "  3.54509758e+00 6.35990610e+00]]\n",
      "\n",
      " A peek at the ground frost labels: \n",
      "[ 9.84928987 10.85267889 12.97189949 ... 21.7275541  23.77582838\n",
      " 17.35386163]\n",
      "\n",
      " A peek at the snow labels: \n",
      "[112.2352382  116.3547495   57.53778808 ... 177.2424627  135.4028786\n",
      " 140.831213  ]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "\n",
    "!ls  /content\n",
    "\n",
    "data_file_full_path = \"/Users/suli/Documents/source/repo/MachineLearning/Week 4/curated_data_1month_2010-2022_nonans.csv\"\n",
    "\n",
    "data_as_list = []\n",
    "\n",
    "# load the dataset\n",
    "with open(data_file_full_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    row_count = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "      if row_count > 0:\n",
    "        data_as_list.append([float(val) for val in row])\n",
    "      row_count += 1\n",
    "    # for row in csv_reader:\n",
    "    #   if row_count > 0:  # Skip the header\n",
    "    #     try:\n",
    "    #         data_as_list.append([float(val) for val in row])\n",
    "    #     except ValueError as e:\n",
    "    #         print(f\"Skipping row {row_count} due to error: {e}\")\n",
    "    # row_count += 1\n",
    "data = numpy.array(data_as_list)\n",
    "\n",
    "# check its shape\n",
    "print(\"\\n The dataset has shape: \"+str(data.shape))\n",
    "\n",
    "\n",
    "# get features and labels from the data\n",
    "# based on the objectives (see the Objectives section)\n",
    "feat_col = [5, 6, 7, 8, 9, 10, 11]\n",
    "ground_frost_col = 4\n",
    "snow_col = 12\n",
    "\n",
    "feats = data[:, feat_col]\n",
    "ground_frost_label = data[:, ground_frost_col]\n",
    "snow_label = data[:, snow_col]\n",
    "\n",
    "\n",
    "# take a peek\n",
    "print(\"\\n A peek at the dataset features: \\n\"+str(feats))\n",
    "print(\"\\n A peek at the ground frost labels: \\n\"+str(ground_frost_label))\n",
    "print(\"\\n A peek at the snow labels: \\n\"+str(snow_label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTfVuu2j20GV"
   },
   "source": [
    "# Section 2 - Split into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "REvSzFN_25S1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_ids = numpy.arange(0, feats.shape[0])\n",
    "\n",
    "random_seed = 1\n",
    "\n",
    "# First randomly split the data into 70:30 to get the training set\n",
    "train_set_ids, rem_set_ids = train_test_split(all_ids, test_size=0.3, train_size=0.7,\n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "\n",
    "\n",
    "# Then further split the remaining data 50:50 into validation and test sets\n",
    "val_set_ids, test_set_ids = train_test_split(rem_set_ids, test_size=0.5, train_size=0.5,\n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "\n",
    "\n",
    "train_data = feats[train_set_ids, :]\n",
    "train_ground_frost_labels = ground_frost_label[train_set_ids]\n",
    "train_snow_labels = snow_label[train_set_ids]\n",
    "\n",
    "val_data = feats[val_set_ids, :]\n",
    "val_ground_frost_labels = ground_frost_label[val_set_ids]\n",
    "val_snow_labels = snow_label[val_set_ids]\n",
    "\n",
    "test_data = feats[test_set_ids, :]\n",
    "test_ground_frost_labels = ground_frost_label[test_set_ids]\n",
    "test_snow_labels = snow_label[test_set_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQPX6zdd3FFr"
   },
   "source": [
    "# Section 3 - Scale (i.e. normalize) the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JvRD8-mX3K9C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A peek at the scaled dataset features: \n",
      "[[ 1.32716655  1.57084147 -0.88330353 ...  1.51819974 -0.59716362\n",
      "  -0.08830932]\n",
      " [ 1.34196535  1.57808582 -0.99442844 ...  1.03005351 -0.70725389\n",
      "   0.32720564]\n",
      " [ 1.32207443  1.53201409 -0.82347666 ... -0.03198748 -0.69392287\n",
      "   0.1237155 ]\n",
      " ...\n",
      " [ 1.03019683 -1.18300764 -1.68424647 ...  0.24956567 -1.67661336\n",
      "   3.16507655]\n",
      " [ 1.2190197  -1.15490117 -1.57817505 ...  0.2289915  -1.91603506\n",
      "   3.37501812]\n",
      " [ 0.04658465 -1.27114612 -1.73722605 ...  3.1470957  -1.30236928\n",
      "   2.4354211 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feats)\n",
    "scaled_feats = scaler.transform(feats)\n",
    "print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_feats))\n",
    "\n",
    "\n",
    "scaled_train_data = scaled_feats[train_set_ids, :]\n",
    "scaled_val_data = scaled_feats[val_set_ids, :]\n",
    "scaled_test_data = scaled_feats[test_set_ids, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly_RGhnf9_c5"
   },
   "source": [
    "# Section 4 - Train and evaluate a perceptron model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4mqO19s2ZH8"
   },
   "source": [
    "\n",
    "* Implement a perceptron from scratch. (See Week 4 lecture for the formal definition, i.e. mathematics, of a perceptron.)\n",
    "* Train and evaluate the perceptron for classification of ground frost label into 2 classes. (See Week 3 lab for preparation of the dataset for classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmlpSj5LnRPG"
   },
   "source": [
    "# Section 5 - Train and evaluate a multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTi7eoHB34Pk"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# A method for computing performance metrics of interest\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmy_metrics\u001b[39m(labels, predictions, show_confusion_matrix\u001b[38;5;241m=\u001b[39m\u001b[43mtrue\u001b[49m):\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m## First work out which class has been predicted for each data sample.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m## Finally return the classification performance\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     predictions_numpy \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     38\u001b[0m     predicted_classes \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39margmax(predictions_numpy, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, accuracy_score\n",
    "\n",
    "### Create the methods to be used\n",
    "\n",
    "# Create the neural network structure\n",
    "# for a 3-layer MLP\n",
    "class three_layer_MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_layer_sizes,\n",
    "                 output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_l1 = nn.Linear(input_size, hidden_layer_sizes[0])\n",
    "        self.hidden_l2 = nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1])\n",
    "        self.output_l3 = nn.Linear(hidden_layer_sizes[1], output_size)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.hidden_l1(inputs)\n",
    "        out = self.hidden_l2(out)\n",
    "        out = self.output_l3(out)\n",
    "        out = torch.softmax(out, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# A method for computing performance metrics of interest\n",
    "def my_metrics(labels, predictions, show_confusion_matrix=True):\n",
    "\n",
    "    ## First work out which class has been predicted for each data sample.\n",
    "    ## Finally return the classification performance\n",
    "    predictions_numpy = predictions.detach().numpy()\n",
    "    predicted_classes = numpy.argmax(predictions_numpy, axis=1)\n",
    "    f1_scores = f1_score(labels, predicted_classes, average=None)\n",
    "    acc = accuracy_score(labels, predicted_classes)\n",
    "\n",
    "    if show_confusion_matrix:\n",
    "      print(\"\\n Confusion matrix:\")\n",
    "      confus_mat = confusion_matrix(labels, predicted_classes)\n",
    "      disp = ConfusionMatrixDisplay(confus_mat)\n",
    "      disp.plot()\n",
    "      plt.show()\n",
    "\n",
    "    return f1_scores, acc\n",
    "\n",
    "\n",
    "# A class for managing the data for training the model\n",
    "class MetDataset(Dataset):\n",
    "    def __init__(self, feats, labels):\n",
    "        # Convert features from numpy arrays to PyTorch tensors\n",
    "        self.feats = torch.tensor(feats, dtype=torch.float32)\n",
    "\n",
    "        # Recode class label -1 to 0\n",
    "        # as the PyTorch library requires class labels to be numbered from zero\n",
    "        numpy.place(labels, labels==-1, 0)\n",
    "\n",
    "        # Convert labels from numpy arrays to PyTorch tensors\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.feats[idx, :], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "67FDfec6g7GS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_frost_label_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m feature_count \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m hidden_layer_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m class_count \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39munique(\u001b[43mground_frost_label_class\u001b[49m)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m MLP_model \u001b[38;5;241m=\u001b[39m three_layer_MLP(feature_count, hidden_layer_sizes, class_count)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Set values for hyperparameters\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ground_frost_label_class' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "### Train and evaluate the 3-layer MLP\n",
    "\n",
    "# Ensure reproducibility\n",
    "# for PyTorch operations that use random numbers internally\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "numpy.random.seed(random_seed)\n",
    "\n",
    "# Create an instance of the 3-layer MLP\n",
    "feature_count = train_data.shape[1]\n",
    "hidden_layer_sizes = [10, 10]\n",
    "class_count = numpy.unique(ground_frost_label_class).shape[0]\n",
    "MLP_model = three_layer_MLP(feature_count, hidden_layer_sizes, class_count)\n",
    "\n",
    "\n",
    "# Set values for hyperparameters\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "# Set up the data loading by batch\n",
    "# With the test and validation sets having only one batch\n",
    "train_set = MetDataset(scaled_train_data, train_ground_frost_labels_class)\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size)\n",
    "\n",
    "val_set = MetDataset(scaled_val_data, val_ground_frost_labels_class)\n",
    "val_dataloader = DataLoader(val_set, batch_size=len(val_set))\n",
    "\n",
    "test_set = MetDataset(scaled_test_data, test_ground_frost_labels_class)\n",
    "test_dataloader = DataLoader(test_set, batch_size=len(test_set))\n",
    "\n",
    "\n",
    "\n",
    "# Set up the SGD optimizer for updating the model weights\n",
    "optimizer = optim.SGD(MLP_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Compute cross entropy loss against the training labels\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "best_model_acc = 0\n",
    "losses = []\n",
    "\n",
    "# Iterate over the dataset at two different stages:\n",
    "# 1. Iterate over the batches in the dataset (inner for loop below)\n",
    "# One complete set of iteration through the dataset (i.e. having gone over\n",
    "# all batches in the dataset at least once) = One epoch\n",
    "# 2. Iterate over the specified numeber of epochs (outer for loop below)\n",
    "for epoch in range(0, num_epochs):\n",
    "\n",
    "    # Set the model to training mode\n",
    "    MLP_model.train()\n",
    "\n",
    "\n",
    "    for batch, (X_train, y_train) in enumerate(train_dataloader):\n",
    "\n",
    "      # Zero out the `.grad` buffers,\n",
    "      # otherwise on the backward pass we'll add the\n",
    "      # new gradients to the old ones.\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Compute the forward pass and then the loss\n",
    "      train_pred = MLP_model.forward(X_train)\n",
    "      train_loss = loss_function(train_pred, y_train)\n",
    "      train_avg_f1_score, train_acc = my_metrics(y_train, train_pred)\n",
    "\n",
    "      # Compute the model parameters' gradients\n",
    "      # and propagating the loss backwards through the network.\n",
    "      train_loss.backward()\n",
    "\n",
    "      # Update the model parameters using those gradients\n",
    "      optimizer.step()\n",
    "\n",
    "    # How well the network does on the batches\n",
    "    # is an indication of how well training is progressing\n",
    "    print(\"epoch: {} - train loss: {:.4f} train acc: {:.4f}\".format(\n",
    "        epoch,\n",
    "        train_loss.item(),\n",
    "        train_acc))\n",
    "\n",
    "    losses.append(train_loss.item())\n",
    "\n",
    "\n",
    "# Finally, test your model on the test set and get an estimate of its performance.\n",
    "# First, set the model to evaluation mode\n",
    "MLP_model.eval()\n",
    "for batch, (X_test, y_test) in enumerate(test_dataloader):\n",
    "  test_pred = MLP_model.forward(X_test)\n",
    "  test_f1_scores, test_accuracy = my_metrics(y_test, test_pred, show_confusion_matrix=True)\n",
    "  print(\"\\n test accuracy: {:2.2f}\".format(test_accuracy))\n",
    "  test_pred_numpy = test_pred.detach().numpy()\n",
    "  print('\\n The F1 scores for each of the classes are: '+str(test_f1_scores))\n",
    "\n",
    "  print(\"\\n Training loss:\")\n",
    "  fig, ax = plt.subplots()\n",
    "  losses = numpy.array(losses)\n",
    "  ax.plot(losses, 'b-', label='training loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18KeTBLI9zJD"
   },
   "source": [
    "# Section 6 - Train the MLP with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tW6JJZR-RxI"
   },
   "source": [
    "* Edit the code in Section 5 to include early stopping. (See Week 4 lecture for information on early stopping.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvTPVuOABC3B"
   },
   "source": [
    "# Section 7 - Explore PyTorch MLP training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdT8laemBQAH"
   },
   "source": [
    "1. The first part of the code in Section 5 uses *torch.nn.Linear()* in building the 3-layer MLP. What do you think that this bit of code does?\n",
    "\n",
    "2. Why was *numpy.argmax(predictions_numpy, axis=1)* needed to get predictions from the MLP? (See the *my_metrics(...)* method of the first part of the Section 5 code.)\n",
    "\n",
    "3. Try different settings for the hyperparameters of the MLP in Section 5, particularly the:\n",
    "  * output layer activation function\n",
    "  * loss function\n",
    "  * number of epochs\n",
    "  * learning rate\n",
    "  * batch size\n",
    "  * optimization algorithm"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP4lRBCg90c9hEZp6bKNNYd",
   "collapsed_sections": [
    "hcgYtozJ1r16",
    "Mgn54mnC2C2F",
    "XTfVuu2j20GV",
    "OQPX6zdd3FFr",
    "Ly_RGhnf9_c5",
    "VmlpSj5LnRPG",
    "18KeTBLI9zJD",
    "JvTPVuOABC3B"
   ],
   "provenance": [
    {
     "file_id": "1rIS3LLDgQ_aPDexPx3EQxFMrwUs7h2cZ",
     "timestamp": 1675782505060
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
