{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rIS3LLDgQ_aPDexPx3EQxFMrwUs7h2cZ","timestamp":1675782505060}],"collapsed_sections":["hcgYtozJ1r16","Mgn54mnC2C2F","XTfVuu2j20GV","OQPX6zdd3FFr","Ly_RGhnf9_c5","VmlpSj5LnRPG","18KeTBLI9zJD","JvTPVuOABC3B"],"authorship_tag":"ABX9TyP4lRBCg90c9hEZp6bKNNYd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Objectives"],"metadata":{"id":"hcgYtozJ1r16"}},{"cell_type":"markdown","source":["\n","* To introduce you to PyTorch (https://pytorch.org/docs/stable/index.html), which is one of the most widely used software libraries for neural networks.\n",">**Remember**: It is your responsibility as a machine learning scientist to read documentations for any library function you use and to thoroughly understand what it is doing, if it validly serves your purpose, and which of its parameters you need to consider.\n","\n","* To apply the perceptron and multilayer perceptron from Week 4 lecture to automatic detection of the number of days of ground frost and snow based on other weather variables."],"metadata":{"id":"4KnN4Z8d1xdQ"}},{"cell_type":"markdown","source":["# Section 1 - Load the UK Met (60km, 2010-2022) data"],"metadata":{"id":"Mgn54mnC2C2F"}},{"cell_type":"markdown","source":["Same as for the Week 3 lab,\n","\n","1. You need to first download the data before you can get started. Download from the Week 3 page for the module, on Canvas (see 'Week 3 Lab Dataset' on the page). The file you download will be named *curated_data_1month_2010-2022_nonans.csv*.\n","\n","2. Then, use the file menu in Google Colab to upload the file to your Colab directory. Once upload is complete, you should be able to see the file on the listed contents of your Colab directory.\n","\n","3. You can now run the code in the cell below to load the data."],"metadata":{"id":"cVH-kE3j2JEC"}},{"cell_type":"code","source":["import csv\n","import numpy\n","\n","\n","!ls  /content\n","\n","data_file_full_path = \"/content/curated_data_1month_2010-2022_nonans.csv\"\n","\n","data_as_list = []\n","\n","# load the dataset\n","with open(data_file_full_path) as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","\n","    row_count = 0\n","    for row in csv_reader:\n","\n","      if row_count > 0:\n","        data_as_list.append([float(val) for val in row])\n","      row_count += 1\n","data = numpy.array(data_as_list)\n","\n","# check its shape\n","print(\"\\n The dataset has shape: \"+str(data.shape))\n","\n","\n","# get features and labels from the data\n","# based on the objectives (see the Objectives section)\n","feat_col = [5, 6, 7, 8, 9, 10, 11]\n","ground_frost_col = 4\n","snow_col = 12\n","\n","feats = data[:, feat_col]\n","ground_frost_label = data[:, ground_frost_col]\n","snow_label = data[:, snow_col]\n","\n","\n","# take a peek\n","print(\"\\n A peek at the dataset features: \\n\"+str(feats))\n","print(\"\\n A peek at the ground frost labels: \\n\"+str(ground_frost_label))\n","print(\"\\n A peek at the snow labels: \\n\"+str(snow_label))\n"],"metadata":{"id":"mUakXgG72Amq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 2 - Split into training, validation, and test sets"],"metadata":{"id":"XTfVuu2j20GV"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","all_ids = numpy.arange(0, feats.shape[0])\n","\n","random_seed = 1\n","\n","# First randomly split the data into 70:30 to get the training set\n","train_set_ids, rem_set_ids = train_test_split(all_ids, test_size=0.3, train_size=0.7,\n","                                 random_state=random_seed, shuffle=True)\n","\n","\n","# Then further split the remaining data 50:50 into validation and test sets\n","val_set_ids, test_set_ids = train_test_split(rem_set_ids, test_size=0.5, train_size=0.5,\n","                                 random_state=random_seed, shuffle=True)\n","\n","\n","train_data = feats[train_set_ids, :]\n","train_ground_frost_labels = ground_frost_label[train_set_ids]\n","train_snow_labels = snow_label[train_set_ids]\n","\n","val_data = feats[val_set_ids, :]\n","val_ground_frost_labels = ground_frost_label[val_set_ids]\n","val_snow_labels = snow_label[val_set_ids]\n","\n","test_data = feats[test_set_ids, :]\n","test_ground_frost_labels = ground_frost_label[test_set_ids]\n","test_snow_labels = snow_label[test_set_ids]"],"metadata":{"id":"REvSzFN_25S1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 3 - Scale (i.e. normalize) the input data"],"metadata":{"id":"OQPX6zdd3FFr"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","scaler.fit(feats)\n","scaled_feats = scaler.transform(feats)\n","print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_feats))\n","\n","\n","scaled_train_data = scaled_feats[train_set_ids, :]\n","scaled_val_data = scaled_feats[val_set_ids, :]\n","scaled_test_data = scaled_feats[test_set_ids, :]"],"metadata":{"id":"JvRD8-mX3K9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 4 - Train and evaluate a perceptron model"],"metadata":{"id":"Ly_RGhnf9_c5"}},{"cell_type":"markdown","source":["\n","* Implement a perceptron from scratch. (See Week 4 lecture for the formal definition, i.e. mathematics, of a perceptron.)\n","* Train and evaluate the perceptron for classification of ground frost label into 2 classes. (See Week 3 lab for preparation of the dataset for classification)"],"metadata":{"id":"h4mqO19s2ZH8"}},{"cell_type":"markdown","source":["# Section 5 - Train and evaluate a multilayer perceptron (MLP)"],"metadata":{"id":"VmlpSj5LnRPG"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","\n","### Create the methods to be used\n","\n","# Create the neural network structure\n","# for a 3-layer MLP\n","class three_layer_MLP(nn.Module):\n","    def __init__(self,\n","                 input_size,\n","                 hidden_layer_sizes,\n","                 output_size):\n","        super().__init__()\n","        self.hidden_l1 = nn.Linear(input_size, hidden_layer_sizes[0])\n","        self.hidden_l2 = nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1])\n","        self.output_l3 = nn.Linear(hidden_layer_sizes[1], output_size)\n","\n","\n","    def forward(self, inputs):\n","        out = self.hidden_l1(inputs)\n","        out = self.hidden_l2(out)\n","        out = self.output_l3(out)\n","        out = torch.softmax(out, 1)\n","        return out\n","\n","\n","# A method for computing performance metrics of interest\n","def my_metrics(labels, predictions, show_confusion_matrix=False):\n","\n","    ## First work out which class has been predicted for each data sample.\n","    ## Finally return the classification performance\n","    predictions_numpy = predictions.detach().numpy()\n","    predicted_classes = numpy.argmax(predictions_numpy, axis=1)\n","    f1_scores = f1_score(labels, predicted_classes, average=None)\n","    acc = accuracy_score(labels, predicted_classes)\n","\n","    if show_confusion_matrix:\n","      print(\"\\n Confusion matrix:\")\n","      confus_mat = confusion_matrix(labels, predicted_classes)\n","      disp = ConfusionMatrixDisplay(confus_mat)\n","      disp.plot()\n","      plt.show()\n","\n","    return f1_scores, acc\n","\n","\n","# A class for managing the data for training the model\n","class MetDataset(Dataset):\n","    def __init__(self, feats, labels):\n","        # Convert features from numpy arrays to PyTorch tensors\n","        self.feats = torch.tensor(feats, dtype=torch.float32)\n","\n","        # Recode class label -1 to 0\n","        # as the PyTorch library requires class labels to be numbered from zero\n","        numpy.place(labels, labels==-1, 0)\n","\n","        # Convert labels from numpy arrays to PyTorch tensors\n","        self.labels = torch.tensor(labels, dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","\n","        return self.feats[idx, :], self.labels[idx]\n"],"metadata":{"id":"MTi7eoHB34Pk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","\n","### Train and evaluate the 3-layer MLP\n","\n","# Ensure reproducibility\n","# for PyTorch operations that use random numbers internally\n","random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","numpy.random.seed(random_seed)\n","\n","# Create an instance of the 3-layer MLP\n","feature_count = train_data.shape[1]\n","hidden_layer_sizes = [10, 10]\n","class_count = numpy.unique(ground_frost_label_class).shape[0]\n","MLP_model = three_layer_MLP(feature_count, hidden_layer_sizes, class_count)\n","\n","\n","# Set values for hyperparameters\n","num_epochs = 100\n","learning_rate = 0.001\n","batch_size = 50\n","\n","\n","# Set up the data loading by batch\n","# With the test and validation sets having only one batch\n","train_set = MetDataset(scaled_train_data, train_ground_frost_labels_class)\n","train_dataloader = DataLoader(train_set, batch_size=batch_size)\n","\n","val_set = MetDataset(scaled_val_data, val_ground_frost_labels_class)\n","val_dataloader = DataLoader(val_set, batch_size=len(val_set))\n","\n","test_set = MetDataset(scaled_test_data, test_ground_frost_labels_class)\n","test_dataloader = DataLoader(test_set, batch_size=len(test_set))\n","\n","\n","\n","# Set up the SGD optimizer for updating the model weights\n","optimizer = optim.SGD(MLP_model.parameters(), lr=learning_rate)\n","\n","\n","# Compute cross entropy loss against the training labels\n","loss_function = nn.CrossEntropyLoss()\n","\n","\n","\n","best_model_acc = 0\n","losses = []\n","\n","# Iterate over the dataset at two different stages:\n","# 1. Iterate over the batches in the dataset (inner for loop below)\n","# One complete set of iteration through the dataset (i.e. having gone over\n","# all batches in the dataset at least once) = One epoch\n","# 2. Iterate over the specified numeber of epochs (outer for loop below)\n","for epoch in range(0, num_epochs):\n","\n","    # Set the model to training mode\n","    MLP_model.train()\n","\n","\n","    for batch, (X_train, y_train) in enumerate(train_dataloader):\n","\n","      # Zero out the `.grad` buffers,\n","      # otherwise on the backward pass we'll add the\n","      # new gradients to the old ones.\n","      optimizer.zero_grad()\n","\n","      # Compute the forward pass and then the loss\n","      train_pred = MLP_model.forward(X_train)\n","      train_loss = loss_function(train_pred, y_train)\n","      train_avg_f1_score, train_acc = my_metrics(y_train, train_pred)\n","\n","      # Compute the model parameters' gradients\n","      # and propagating the loss backwards through the network.\n","      train_loss.backward()\n","\n","      # Update the model parameters using those gradients\n","      optimizer.step()\n","\n","    # How well the network does on the batches\n","    # is an indication of how well training is progressing\n","    print(\"epoch: {} - train loss: {:.4f} train acc: {:.4f}\".format(\n","        epoch,\n","        train_loss.item(),\n","        train_acc))\n","\n","    losses.append(train_loss.item())\n","\n","\n","# Finally, test your model on the test set and get an estimate of its performance.\n","# First, set the model to evaluation mode\n","MLP_model.eval()\n","for batch, (X_test, y_test) in enumerate(test_dataloader):\n","  test_pred = MLP_model.forward(X_test)\n","  test_f1_scores, test_accuracy = my_metrics(y_test, test_pred, show_confusion_matrix=True)\n","  print(\"\\n test accuracy: {:2.2f}\".format(test_accuracy))\n","  test_pred_numpy = test_pred.detach().numpy()\n","  print('\\n The F1 scores for each of the classes are: '+str(test_f1_scores))\n","\n","  print(\"\\n Training loss:\")\n","  fig, ax = plt.subplots()\n","  losses = numpy.array(losses)\n","  ax.plot(losses, 'b-', label='training loss')\n"],"metadata":{"id":"67FDfec6g7GS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 6 - Train the MLP with early stopping"],"metadata":{"id":"18KeTBLI9zJD"}},{"cell_type":"markdown","source":["* Edit the code in Section 5 to include early stopping. (See Week 4 lecture for information on early stopping.)"],"metadata":{"id":"-tW6JJZR-RxI"}},{"cell_type":"markdown","source":["# Section 7 - Explore PyTorch MLP training and evaluation"],"metadata":{"id":"JvTPVuOABC3B"}},{"cell_type":"markdown","source":["1. The first part of the code in Section 5 uses *torch.nn.Linear()* in building the 3-layer MLP. What do you think that this bit of code does?\n","\n","2. Why was *numpy.argmax(predictions_numpy, axis=1)* needed to get predictions from the MLP? (See the *my_metrics(...)* method of the first part of the Section 5 code.)\n","\n","3. Try different settings for the hyperparameters of the MLP in Section 5, particularly the:\n","  * output layer activation function\n","  * loss function\n","  * number of epochs\n","  * learning rate\n","  * batch size\n","  * optimization algorithm"],"metadata":{"id":"sdT8laemBQAH"}}]}