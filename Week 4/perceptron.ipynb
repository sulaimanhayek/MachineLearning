{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron code only\n",
    "In this notebook, I will be working on the perceptron code only. We will start with the prediction function first and feed in the features and the corresponding weights and bias.\n",
    "\n",
    "Perceptron is the building block of the neural network, as mentioned in the playground notebook it do simple classification by drawing a straight line that seperates the classified features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row)-1):\n",
    "        activation += weights[i+1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predictions\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "weights = [-0.1, 0.20653640140000007, -0.23418117710000003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n"
     ]
    }
   ],
   "source": [
    "for row in dataset:\n",
    "\tprediction = predict(row, weights)\n",
    "\tprint(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two inputs values (X1 and X2) and three weight values (bias, w1 and w2). The activation equation we have modeled for this problem is:\n",
    "\n",
    "activation = (w1 * X1) + (w2 * X2) + bias\n",
    "\n",
    "Or, with the specific weight values we chose by hand as:\n",
    "\n",
    "activation = (0.206 * X1) + (-0.234 * X2) + -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0.0\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.100, error=2.000\n",
      ">epoch=1, lrate=0.100, error=1.000\n",
      ">epoch=2, lrate=0.100, error=0.000\n",
      ">epoch=3, lrate=0.100, error=0.000\n",
      ">epoch=4, lrate=0.100, error=0.000\n",
      "[-0.1, 0.20653640140000007, -0.23418117710000003]\n"
     ]
    }
   ],
   "source": [
    "l_rate = 0.1\n",
    "n_epoch = 5\n",
    "weights = train_weights(dataset, l_rate, n_epoch)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test on Sonar dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [82.6923076923077, 68.26923076923077]\n",
      "Mean Accuracy: 75.481%\n"
     ]
    }
   ],
   "source": [
    "# Perceptron Algorithm on the Sonar Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    " \n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\treturn weights\n",
    " \n",
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "\tpredictions = list()\n",
    "\tweights = train_weights(train, l_rate, n_epoch)\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(row, weights)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n",
    " \n",
    "# Test the Perceptron algorithm on the sonar dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'sonar.all-data.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert string class to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 2\n",
    "l_rate = 0.01\n",
    "n_epoch = 500\n",
    "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
