{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4: Supervised Learning III\n",
    "### Neural Networks\n",
    "So, in this week we will be using pytorch, one of the best libraries for neural networks.\n",
    "* To apply the perceptron and multilayer perceptron from Week 4 lecture to automatic detection of the number of days of ground frost and snow based on other weather variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve the above goal, we'll need first to seperate the dataset into training and testing. We'll apply 70:30 split and then further split testing into validation:test as 50:50.\n",
    "\n",
    "We'll first import the csv file, read it and then apply seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10296, 13)\n",
      "['8' '4' '1' '1' '9.84928987' '89.36982749' '1022.665365' '64.51156417'\n",
      " '9.12500556' '6.45810733' '6.727447717' '0.697199793' '112.2352382']\n",
      "\n",
      " A peek at the dataset features: \n",
      "[['89.36982749' '1022.665365' '64.51156417' ... '6.45810733'\n",
      "  '6.727447717' '112.2352382']\n",
      " ['89.44621093' '1022.708003' '57.48681167' ... '5.881910517'\n",
      "  '6.230648281' '116.3547495']\n",
      " ['89.34354469' '1022.436839' '68.29351494' ... '4.628301269'\n",
      "  '6.290806564' '57.53778808']\n",
      " ...\n",
      " ['87.83702931' '1006.457057' '13.8800195' ... '4.960640256'\n",
      "  '1.856263012' '177.2424627']\n",
      " ['88.81163154' '1006.622483' '20.58531624' ... '4.936354968'\n",
      "  '0.775835354' '135.4028786']\n",
      " ['82.76015165' '1005.938301' '10.53091935' ... '8.380819417'\n",
      "  '3.545097582' '140.831213']]\n",
      "\n",
      " A peek at the ground frost labels: \n",
      "['9.84928987' '10.85267889' '12.97189949' ... '21.7275541' '23.77582838'\n",
      " '17.35386163']\n",
      "\n",
      " A peek at the snow labels: \n",
      "['0.697199793' '1.629525681' '1.172937726' ... '7.997095434' '8.468158997'\n",
      " '6.3599061']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "file_path = \"/Users/suli/Documents/source/repo/MachineLearning/Week 4/curated_data_1month_2010-2022_nonans.csv\"\n",
    "\n",
    "data_list = []\n",
    "with open(file_path, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    row_count = 0\n",
    "    for row in reader:\n",
    "        if row_count > 0:\n",
    "            data_list.append(row)\n",
    "        row_count += 1\n",
    "        \n",
    "data = np.array(data_list)\n",
    "print(data.shape)\n",
    "print(data[0])\n",
    "\n",
    "feat_col = [5, 6, 7, 8, 9, 10, 12]\n",
    "ground_frost_col = 4\n",
    "snow_col = 11 # should be 11 for snow and 12 for rain\n",
    "\n",
    "feats = data[:, feat_col]\n",
    "ground_frost_label = data[:, ground_frost_col]\n",
    "snow_label = data[:, snow_col]\n",
    "\n",
    "print(\"\\n A peek at the dataset features: \\n\"+str(feats))\n",
    "print(\"\\n A peek at the ground frost labels: \\n\"+str(ground_frost_label))\n",
    "print(\"\\n A peek at the snow labels: \\n\"+str(snow_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so now we will be working on splitting into training and validation and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 10293 10294 10295]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_ids = np.arange(feats.shape[0])\n",
    "print(all_ids)\n",
    "\n",
    "random_seed =1\n",
    "\n",
    "train_set_id, rem_set_id = train_test_split(all_ids, test_size=0.3, train_size=0.7, random_state=random_seed, shuffle=True)\n",
    "\n",
    "validate_set_id, test_set_id = train_test_split(rem_set_id, test_size=0.5, train_size=0.5, random_state=random_seed, shuffle=True)\n",
    "\n",
    "train_set_data = feats[train_set_id,:]\n",
    "train_ground_frost_labels = ground_frost_label[train_set_id]\n",
    "train_snow_labels = snow_label[train_set_id]\n",
    "\n",
    "validate_set_data = feats[validate_set_id,:]\n",
    "validate_ground_frost_labels = ground_frost_label[validate_set_id]\n",
    "validate_snow_labels = snow_label[validate_set_id]\n",
    "\n",
    "test_set_data = feats[test_set_id,:]\n",
    "test_ground_frost_labels = ground_frost_label[test_set_id]\n",
    "test_snow_labels = snow_label[test_set_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now we have all of our data ready but before moving to the next step. it's better to scale our training data such that all datapoints are on a 0-1 scale. This makes our neural network perform better. Simply because If one feature has values in the range of [0,1], and another feature is in the range of [1000,5000], the model may assign more importance to the larger values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A peek at the scaled dataset features: \n",
      "[[ 1.32716655  1.57084147 -0.88330353 ...  1.51819974 -0.59716362\n",
      "   0.21295927]\n",
      " [ 1.34196535  1.57808582 -0.99442844 ...  1.03005351 -0.70725389\n",
      "   0.27324641]\n",
      " [ 1.32207443  1.53201409 -0.82347666 ... -0.03198748 -0.69392287\n",
      "  -0.58751254]\n",
      " ...\n",
      " [ 1.03019683 -1.18300764 -1.68424647 ...  0.24956567 -1.67661336\n",
      "   1.16430985]\n",
      " [ 1.2190197  -1.15490117 -1.57817505 ...  0.2289915  -1.91603506\n",
      "   0.55200694]\n",
      " [ 0.04658465 -1.27114612 -1.73722605 ...  3.1470957  -1.30236928\n",
      "   0.6314481 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feats)\n",
    "scaled_feats = scaler.transform(feats)\n",
    "print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_feats))\n",
    "\n",
    "\n",
    "scaled_train_data = scaled_feats[train_set_id, :]\n",
    "scaled_val_data = scaled_feats[validate_set_id, :]\n",
    "scaled_test_data = scaled_feats[test_set_id, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now we're ready to train our perceptron!\n",
    "\n",
    "So we will start by the multiplication of the x and w. Where:\n",
    "- x: input features (data points as a matrix)\n",
    "- w: weight vector (learned parameters of the perceptron)\n",
    "- the dot product xw gives a raw score that determines classification.\n",
    "\n",
    "After this we will be applying ativation function (sign(xw))\n",
    "- The perceptron uses a step function as an activation function\n",
    "- The function determines the class based on whether xw is positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a perceptron from scratch.\n",
    "def predict (x,w):\n",
    "  # compute the dot product of x and w\n",
    "  y_cap = np.matmul(x,w)\n",
    "  # Compute the activation sign(xw)\n",
    "  np.place(y_cap, y_cap>=0, 1)\n",
    "  np.place(y_cap, y_cap<0,-1)\n",
    "  return y_cap\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
