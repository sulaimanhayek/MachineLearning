{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhcT91UwAajV"
   },
   "source": [
    "# Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAV_surPsiiZ"
   },
   "source": [
    "* To explore the convolutional and long short-term neural networks from Week 8 lecture and apply them for forecast of number of days of ground frost and snow based on other weather variables.\n",
    "> **Remember**: It is your responsibility as a machine learning scientist to read documentations for each library function in the code to thoroughly understand what it is doing, how it serves the purpose highlighted in the code comments, and other parameters that could be set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T0C9f64XN4z"
   },
   "source": [
    "# Section 1 - Load new UK Met (60km, 2010-2022) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3qjCVVGXPhT"
   },
   "source": [
    "Although the same UK Met dataset will be used, the format of the dataset for this week is different from that in Week 3.\n",
    "\n",
    "1. You need to first download the data before you can get started. Download from the Week 8 page for the module, on Canvas (see 'Week 8 Lab Dataset' on the page). The file you download will be named *curated_data_24month_2010-2022_nonans.csv*.\n",
    "\n",
    "2. Explore the dataset (see Section 1 of the Week 3 lab notebook for where to get started).\n",
    "\n",
    "3. Then, use the file menu in Google Colab to upload the file to your Colab directory. Once upload is complete, you should be able to see the file on the listed contents of your Colab directory.\n",
    "\n",
    "4. You can now run the code in the cell below to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ieQg8pLsYU1z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /content: No such file or directory\n",
      "\n",
      " The dataset has shape: (8502, 220)\n",
      "\n",
      " The reshaped data has shape: (8502, 23, 7)\n",
      "\n",
      " A peek at the dataset features: \n",
      "[[[  89.36982749   88.64096898 1019.367542   ...   14.16234929\n",
      "      4.20105888   69.94420521]\n",
      "  [  87.17137766 1022.665365   1014.037188   ...   13.56425092\n",
      "      4.69816926   45.82438428]\n",
      "  [  82.69586138 1030.782071     64.51156417 ...    9.14180954\n",
      "      5.90909077    9.12500556]\n",
      "  ...\n",
      "  [  87.63357222 1018.310337    317.1626879  ...    5.66678355\n",
      "      7.16431878   15.57581082]\n",
      "  [  90.33110135 1016.221761    200.1635946  ...    5.21315437\n",
      "      9.54317305   15.43351136]\n",
      "  [  85.49765794 1010.421393    117.0037764  ...    3.96274573\n",
      "     13.51167177   14.16234929]]\n",
      "\n",
      " [[  89.44621093   88.68141089 1019.222602   ...   13.71516642\n",
      "      4.05930168   69.52167643]\n",
      "  [  87.15124439 1022.708003   1014.449862   ...   13.28010743\n",
      "      4.05530053   42.21724   ]\n",
      "  [  82.37861266 1030.9105       57.48681167 ...    8.8730343\n",
      "      6.03667066    8.81473207]\n",
      "  ...\n",
      "  [  85.93522642 1018.36367     296.5599508  ...    5.51301789\n",
      "      6.95868097   15.21114667]\n",
      "  [  89.81374238 1016.269668    182.9731359  ...    4.99251201\n",
      "      9.67430105   15.01253119]\n",
      "  [  85.9396577  1010.769981    103.4041753  ...    3.71880687\n",
      "     13.42604207   13.71516642]]\n",
      "\n",
      " [[  89.34354469   88.49552335 1018.866494   ...   13.60508591\n",
      "      3.50596766   70.45523694]\n",
      "  [  87.18630077 1022.436839   1014.730447   ...   13.18702045\n",
      "      3.18684069   50.59185951]\n",
      "  [  82.85678752 1030.740929     68.29351494 ...    8.8651609\n",
      "      4.93636082    8.70098793]\n",
      "  ...\n",
      "  [  85.00435065 1018.225261    289.4399061  ...    4.4200344\n",
      "      7.1721332    15.05985271]\n",
      "  [  88.76614673 1016.165036    189.0363267  ...    4.41496054\n",
      "     10.31616049   14.80957151]\n",
      "  [  86.31635639 1010.997181     99.28759317 ...    3.41442245\n",
      "     13.78566257   13.60508591]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  88.48627714   87.83702931  997.6120581  ...   11.64771616\n",
      "      4.84966896   38.78644538]\n",
      "  [  77.10145419 1008.882859   1006.457057   ...   10.08009405\n",
      "      5.78072932   13.8800195 ]\n",
      "  [  83.613992   1010.105661     33.37136435 ...    8.6763326\n",
      "      6.52474008    5.60077349]\n",
      "  ...\n",
      "  [  84.75400368 1015.512995     85.81209814 ...    6.05830716\n",
      "      6.6310562    12.40012695]\n",
      "  [  83.51635451 1012.479903    115.7976168  ...    5.95184988\n",
      "      9.72105639   12.33118117]\n",
      "  [  84.56115974 1004.270237     74.55278946 ...    5.21960935\n",
      "     12.23828854   11.64771616]]\n",
      "\n",
      " [[  89.41717429   88.81163154  998.3234867  ...   11.64516553\n",
      "      4.18830573   48.56508744]\n",
      "  [  77.94633497 1008.790648   1006.622483   ...    9.98338964\n",
      "      5.7982141    20.58531624]\n",
      "  [  81.4275747  1010.960868     44.89192737 ...    8.50677591\n",
      "      5.88731305    5.50496598]\n",
      "  ...\n",
      "  [  84.90170054 1015.566655    100.4781688  ...    5.66397372\n",
      "      6.05363835   12.32072462]\n",
      "  [  83.21899791 1012.474231    136.8665038  ...    5.56694939\n",
      "      9.65802725   12.18080031]\n",
      "  [  85.56666267 1004.696915     84.72539677 ...    4.93842435\n",
      "     12.27670908   11.64516553]]\n",
      "\n",
      " [[  84.71638935   82.76015165 1001.415048   ...   11.64624474\n",
      "      6.08884932   36.66748131]\n",
      "  [  81.790533   1006.55981    1005.938301   ...   10.6583103\n",
      "      7.17482847   10.53091935]\n",
      "  [  84.64270043 1013.222467     47.87755143 ...   10.05827155\n",
      "      8.28850101    6.25010021]\n",
      "  ...\n",
      "  [  86.73388894 1015.124441     95.3488628  ...    7.40622476\n",
      "      5.69180054   12.1540789 ]\n",
      "  [  87.18749662 1012.627101    102.2147084  ...    7.39382032\n",
      "      9.31663618   12.48612746]\n",
      "  [  87.64294422 1004.410619    112.9731221  ...    6.97038755\n",
      "     11.50842849   11.64624474]]]\n",
      "\n",
      " A peek at the ground frost labels: \n",
      "[ 8.37843641  7.90887357 12.51721277 ... 21.7275541  23.77582838\n",
      " 17.35386163]\n",
      "\n",
      " A peek at the snow labels: \n",
      "[100.5669088   94.28527309  95.75413462 ...  55.58289579  72.80360827\n",
      "  66.85270693]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "\n",
    "!ls  /content\n",
    "\n",
    "data_file_full_path = \"/Users/suli/Documents/source/repo/MachineLearning/Week 8/curated_data_24month_2010-2022_nonans.csv\"\n",
    "\n",
    "data_as_list = []\n",
    "\n",
    "# load the dataset\n",
    "with open(data_file_full_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    row_count = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "      if row_count > 0:\n",
    "        data_as_list.append([float(val) for val in row])\n",
    "      row_count += 1\n",
    "data = numpy.array(data_as_list)\n",
    "\n",
    "# check its shape\n",
    "print(\"\\n The dataset has shape: \"+str(data.shape))\n",
    "\n",
    "\n",
    "# get features and labels from the data\n",
    "# based on the objectives (see the Objectives section)\n",
    "ground_frost_24_col = 27\n",
    "hurs_1_to_23_col = numpy.arange(ground_frost_24_col+1, ground_frost_24_col+24)\n",
    "psl_1_to_23_col = hurs_1_to_23_col + 23\n",
    "sun_1_to_23_col = psl_1_to_23_col + 23\n",
    "pv_1_to_23_col = sun_1_to_23_col + 23\n",
    "wind_1_to_23_col = pv_1_to_23_col + 23\n",
    "tas_1_to_23_col = wind_1_to_23_col + 23\n",
    "snow_24_col = 97\n",
    "rain_1_to_23_col = numpy.arange(snow_24_col+1, snow_24_col+24)\n",
    "\n",
    "# arrange to shape N x T X D\n",
    "# where N = number of data instances\n",
    "# T = number of timesteps\n",
    "# D = number of variables\n",
    "feats = numpy.stack((data[:, hurs_1_to_23_col], data[:, psl_1_to_23_col],\n",
    "                     data[:, sun_1_to_23_col], data[:, pv_1_to_23_col],\n",
    "                     data[:, wind_1_to_23_col], data[:, tas_1_to_23_col],\n",
    "                     data[:, rain_1_to_23_col]))\n",
    "feats = numpy.transpose(feats, (1, 2, 0))\n",
    "ground_frost_label = data[:, ground_frost_24_col]\n",
    "snow_label = data[:, snow_24_col]\n",
    "\n",
    "# check its shape\n",
    "print(\"\\n The reshaped data has shape: \"+str(feats.shape))\n",
    "\n",
    "# take a peek\n",
    "print(\"\\n A peek at the dataset features: \\n\"+str(feats))\n",
    "print(\"\\n A peek at the ground frost labels: \\n\"+str(ground_frost_label))\n",
    "print(\"\\n A peek at the snow labels: \\n\"+str(snow_label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZpPOFRBefCA"
   },
   "source": [
    "# Section 2 - Split into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4alBjB0JegjD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_ids = numpy.arange(0, feats.shape[0])\n",
    "\n",
    "random_seed = 1\n",
    "\n",
    "# First randomly split the data into 70:30 to get the training set\n",
    "train_set_ids, rem_set_ids = train_test_split(all_ids, test_size=0.3, train_size=0.7,\n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "\n",
    "\n",
    "# Then further split the remaining data 50:50 into validation and test sets\n",
    "val_set_ids, test_set_ids = train_test_split(rem_set_ids, test_size=0.5, train_size=0.5,\n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "\n",
    "\n",
    "train_ground_frost_labels = ground_frost_label[train_set_ids]\n",
    "train_snow_labels = snow_label[train_set_ids]\n",
    "\n",
    "val_ground_frost_labels = ground_frost_label[val_set_ids]\n",
    "val_snow_labels = snow_label[val_set_ids]\n",
    "\n",
    "test_ground_frost_labels = ground_frost_label[test_set_ids]\n",
    "test_snow_labels = snow_label[test_set_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usPVA1hceoh2"
   },
   "source": [
    "# Section 3 - Scale (i.e. normalize) the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BPCnbQmmeqSj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A peek at the scaled dataset features: \n",
      "[[[0.80064506 0.02645592 0.99150722 ... 0.82249309 0.26487964 0.19439118]\n",
      "  [0.73380683 0.9914812  0.98630473 ... 0.78168679 0.2874864  0.12383332]\n",
      "  [0.59774024 0.99986731 0.05955527 ... 0.47995803 0.34255468 0.01647639]\n",
      "  ...\n",
      "  [0.74785867 0.98698163 0.30614608 ... 0.24286834 0.39963786 0.03534698]\n",
      "  [0.82987012 0.98482373 0.19195343 ... 0.21191869 0.50781945 0.03493071]\n",
      "  [0.68292168 0.97883085 0.11078836 ... 0.12660739 0.68829226 0.03121217]]\n",
      "\n",
      " [[0.8029673  0.02649771 0.99136576 ... 0.79198326 0.25843304 0.19315516]\n",
      "  [0.73319473 0.99152526 0.9867075  ... 0.76230061 0.25825108 0.11328132]\n",
      "  [0.58809511 1.         0.05269902 ... 0.46162037 0.34835655 0.01556875]\n",
      "  ...\n",
      "  [0.69622483 0.98703673 0.28603754 ... 0.23237741 0.3902862  0.03428022]\n",
      "  [0.81414115 0.98487323 0.17517532 ... 0.19686499 0.51378267 0.03369921]\n",
      "  [0.69635955 0.979191   0.09751497 ... 0.10996424 0.68439813 0.02990402]]\n",
      "\n",
      " [[0.799846   0.02630565 0.9910182  ... 0.78447283 0.23326943 0.19588611]\n",
      "  [0.73426053 0.99124509 0.98698136 ... 0.75594959 0.2187567  0.13777964]\n",
      "  [0.60263279 0.9998248  0.0632465  ... 0.4610832  0.29831848 0.01523601]\n",
      "  ...\n",
      "  [0.66792394 0.98689373 0.27908828 ... 0.15780671 0.39999323 0.03383764]\n",
      "  [0.78229169 0.98476512 0.18109307 ... 0.15746053 0.54297209 0.03310549]\n",
      "  [0.7078121  0.97942574 0.09349713 ... 0.08919708 0.70075236 0.029582  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.77378298 0.0256253  0.97027359 ... 0.65092787 0.29437605 0.10324519]\n",
      "  [0.42765665 0.97724125 0.97890642 ... 0.54397412 0.33671727 0.03038627]\n",
      "  [0.62565365 0.97850463 0.02916203 ... 0.44820006 0.37055216 0.00616694]\n",
      "  ...\n",
      "  [0.66031278 0.98409144 0.08034487 ... 0.26958072 0.37538703 0.02605713]\n",
      "  [0.62268523 0.98095768 0.10961113 ... 0.26231748 0.51590893 0.02585544]\n",
      "  [0.65444985 0.97247553 0.06935564 ... 0.2123591  0.63038345 0.0238561 ]]\n",
      "\n",
      " [[0.80208452 0.02663225 0.97096795 ... 0.65075385 0.26429967 0.13185071]\n",
      "  [0.45334308 0.97714598 0.97906788 ... 0.5373763  0.33751242 0.05000131]\n",
      "  [0.55918124 0.97938823 0.04040625 ... 0.43663176 0.34156431 0.00588667]\n",
      "  ...\n",
      "  [0.66480312 0.98414688 0.09465915 ... 0.24267663 0.34912818 0.02582485]\n",
      "  [0.61364487 0.98095182 0.13017464 ... 0.23605698 0.5130426  0.02541553]\n",
      "  [0.68501959 0.97291637 0.07928424 ... 0.19317476 0.63213067 0.02384863]]\n",
      "\n",
      " [[0.65916921 0.02037992 0.97398536 ... 0.65082748 0.35072944 0.09704657]\n",
      "  [0.57021606 0.97484109 0.97840011 ... 0.58342393 0.4001158  0.02058912]\n",
      "  [0.65692889 0.98172489 0.04332026 ... 0.54248525 0.45076156 0.00806642]\n",
      "  ...\n",
      "  [0.72050611 0.98368999 0.08965288 ... 0.36154474 0.33267312 0.02533736]\n",
      "  [0.73429689 0.98110976 0.09635403 ... 0.36069842 0.49751738 0.0263087 ]\n",
      "  [0.7481436  0.97262057 0.10685439 ... 0.33180899 0.59719208 0.02385179]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# note that unlike previous weeks\n",
    "# a normalization is to 0-1 range here\n",
    "scaled_feats = numpy.reshape(feats, (-1, feats.shape[2]))\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(scaled_feats)\n",
    "scaled_feats = scaler.transform(scaled_feats)\n",
    "scaled_feats = numpy.reshape(scaled_feats, (feats.shape[0], feats.shape[1], feats.shape[2]))\n",
    "print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_feats))\n",
    "\n",
    "scaled_train_data = scaled_feats[train_set_ids, :]\n",
    "scaled_val_data = scaled_feats[val_set_ids, :]\n",
    "scaled_test_data = scaled_feats[test_set_ids, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tOkWGkVs3Uo"
   },
   "source": [
    "# Section 4 - Train a convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iusYY-PFROe2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Creating the network structure\n",
    "# for a 3-layer CNN\n",
    "class three_layer_CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_layer_channel_sizes,\n",
    "                 output_size, input_channel_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_c1 = nn.Conv1d(input_channel_size, hidden_layer_channel_sizes[0], 6, stride=1)\n",
    "        self.hidden_m1 = (nn.MaxPool1d(3, stride=1))\n",
    "        self.hidden_c2 = nn.Conv1d(hidden_layer_channel_sizes[0], hidden_layer_channel_sizes[1], 4, stride=1)\n",
    "        self.hidden_m2 = nn.MaxPool1d(3, stride=1)\n",
    "        self.hidden_c3 = nn.Conv1d(hidden_layer_channel_sizes[1], hidden_layer_channel_sizes[2], 3, stride=1)\n",
    "        self.hidden_m3 = nn.MaxPool1d(3, stride=1)\n",
    "        self.output = nn.Linear(output_size[0], output_size[1])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.hidden_c1(torch.permute(inputs, (0, 2, 1)))\n",
    "        out = self.hidden_m1(self.relu(out))\n",
    "        out = self.hidden_c2(out)\n",
    "        out = self.hidden_m2(self.relu(out))\n",
    "        out = self.hidden_c3(out)\n",
    "        out = self.hidden_m3(self.relu(out))\n",
    "        out = torch.reshape(out, (out.size(0), -1))\n",
    "        out = self.output(self.relu(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# a method for training a model\n",
    "def train_model(model,\n",
    "                train_set, val_set,\n",
    "                epochs, learn_rate):\n",
    "\n",
    "  # Setting up the SGD optimizer for updating the model weights\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learn_rate)\n",
    "\n",
    "\n",
    "  # Computing cross entropy loss against the training labels\n",
    "  loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "  best_model_mae = sys.float_info.max\n",
    "  losses = []\n",
    "\n",
    "  # Iterating over the dataset at two different staages:\n",
    "  # 1. Iterating over the batches in the dataset (inner for loop below)\n",
    "  # One complete set of iteration through the dataset (i.e. having gone over\n",
    "  # all batches in the dataset at least once) = One epoch\n",
    "  # 2. Iterating over the specified numeber of epochs (outer for loop below)\n",
    "  for epoch in range(0, epochs):\n",
    "\n",
    "      # Setting the model to training mode\n",
    "      model.train()\n",
    "\n",
    "      if epoch == 0:  best_model = deepcopy(model)\n",
    "\n",
    "      for batch, (X_train, y_train) in enumerate(train_set):\n",
    "\n",
    "        # Zeroing out the `.grad` buffers,\n",
    "        # otherwise on the backward pass we'll add the\n",
    "        # new gradients to the old ones.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Computing the forward pass and then the loss\n",
    "        train_pred = model.forward(X_train)\n",
    "        train_loss = loss_function(train_pred, y_train)\n",
    "        train_pred_numpy = train_pred.detach().numpy()\n",
    "        train_mae = mean_absolute_error(y_train[:], train_pred_numpy[:])\n",
    "\n",
    "        # Computing the model parameters' gradients\n",
    "        # and propagating the loss backwards through the network.\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Updating the model parameters using those gradients\n",
    "        optimizer.step()\n",
    "\n",
    "      # Evaluating on the validation set\n",
    "      model.eval()\n",
    "      for batch, (X_val, y_val) in enumerate(val_set):\n",
    "        val_pred = model.forward(X_val)\n",
    "        val_loss = loss_function(val_pred, y_val)\n",
    "        val_pred_numpy = val_pred.detach().numpy()\n",
    "        val_mae = mean_absolute_error(y_val[:], val_pred_numpy[:])\n",
    "\n",
    "      if val_mae <= best_model_mae:\n",
    "        best_model_mae = val_mae\n",
    "        best_model = deepcopy(model)\n",
    "        print('Found improvement in performance. New model saved.')\n",
    "\n",
    "      # How well the network does at the end of an epoch\n",
    "      # is an indication of how well training is progressing\n",
    "      print(\"epoch: {} - train loss: {:.4f} train mae: {:.2f} val loss: {:.4f} val mae: {:.2f}\".format(\n",
    "          epoch,\n",
    "          train_loss.item(),\n",
    "          train_mae,\n",
    "          val_loss.item(),\n",
    "          val_mae))\n",
    "\n",
    "      losses.append([train_loss.item(), val_loss.item()])\n",
    "\n",
    "  return best_model, losses\n",
    "\n",
    "\n",
    "# a method for evaluating a model\n",
    "# including the metrics earlier prepared\n",
    "# and a graph of the above loss per epoch\n",
    "def evaluate_model(model,\n",
    "                test_set,\n",
    "                training_losses, plot=True, text=''):\n",
    "\n",
    "  model.eval()\n",
    "  for batch, (X_test, y_test) in enumerate(test_set):\n",
    "    test_pred = model.forward(X_test)\n",
    "    test_pred_numpy = test_pred.detach().numpy()\n",
    "    test_mae = mean_absolute_error(y_test[:], test_pred_numpy[:])\n",
    "    print(\"\\n Mean absolute error on the test set{:s}: {:2.2f}\".format(text, test_mae))\n",
    "    print()\n",
    "\n",
    "  if plot:\n",
    "    fig, ax = plt.subplots()\n",
    "    training_losses = numpy.array(training_losses)\n",
    "    ax.plot(training_losses[:, 0], 'b-', label='training loss')\n",
    "    ax.plot(training_losses[:, 1], 'k-', label='validation loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.show\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A class for managing the data for training the model\n",
    "class MetDataset(Dataset):\n",
    "    def __init__(self, feats, labels):\n",
    "        # Convert features from numpy arrays to PyTorch tensors\n",
    "        self.feats = torch.tensor(feats, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        # Convert labels from numpy arrays to PyTorch tensors\n",
    "        self.labels = torch.tensor(numpy.reshape(labels, (-1, 1)), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.feats[idx, :], self.labels[idx, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjDHm6lxbAta"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# To ensure reproducibility\n",
    "# for PyTorch operations that use random numbers internally\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "numpy.random.seed(random_seed)\n",
    "\n",
    "# Create an instance of the CNN network\n",
    "input_channel_size_cnn = 7\n",
    "hidden_layer_sizes_cnn = [7, 30, 7]\n",
    "output_sizes_cnn = [hidden_layer_sizes_cnn[2]*7, 1]\n",
    "model_CNN = three_layer_CNN(hidden_layer_sizes_cnn, output_sizes_cnn, input_channel_size_cnn)\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "num_epochs_CNN = 100\n",
    "learning_rate_CNN = 0.0005\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "# Set up the data loading by batches\n",
    "# With the test and validation sets having only one batch\n",
    "train_set = MetDataset(scaled_train_data, train_ground_frost_labels)\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size)\n",
    "\n",
    "val_set = MetDataset(scaled_val_data, val_ground_frost_labels)\n",
    "val_dataloader = DataLoader(val_set, batch_size=len(val_set))\n",
    "\n",
    "test_set = MetDataset(scaled_test_data, test_ground_frost_labels)\n",
    "test_dataloader = DataLoader(test_set, batch_size=len(test_set))\n",
    "\n",
    "\n",
    "\n",
    "# train the model\n",
    "model_CNN, train_val_losses_CNN = train_model(model_CNN, train_dataloader,\n",
    "                                                          val_dataloader,\n",
    "                                                          num_epochs_CNN,\n",
    "                                                          learning_rate_CNN)\n",
    "\n",
    "# evaluate the model\n",
    "evaluate_model(model_CNN, test_dataloader, train_val_losses_CNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBXvhWxVDD_h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlPl2L0ybPkN"
   },
   "source": [
    "# Section 5 - Explore the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ndUxf-Hbbov"
   },
   "source": [
    "* For each layer of the CNN in Section 4, calculate the dimension of the output of the layer. (Hint: See Week 8 lecture slides for the formulae for computing the output dimension of a convolutional layer. The same formula applies to pooling layers.)\n",
    "\n",
    "* Based on your calculations above and the structure of the CNN, how many parameters does the model have? How does this compare with the number of parameters if each convolutional layer was instead a fully connected layer.\n",
    "\n",
    "* Perform data augmentation (see Week 5 lecture), e.g.\n",
    "  * dropout along the time dimension,\n",
    "  * dropout along the variable dimension,\n",
    "  * random noise along the time dimension.\n",
    "\n",
    "* How does the augmentation affect generalisation of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwRcnzOJcWGG"
   },
   "source": [
    "# Section 6 - Train a long short-term memory neural network (LSTMNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hndEuTUQbta1"
   },
   "source": [
    "* Create and train a three-layer LSTMNN model (see https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html for the documentation for the PyTorch LSTM layer)\n",
    "\n",
    "* How many weights does the LSTMNN have?\n",
    "\n",
    "* How does your model perform in comparison with the CNN in Section 4?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNxDYGLRilTxpOS1429RLpp",
   "collapsed_sections": [
    "lhcT91UwAajV",
    "_T0C9f64XN4z",
    "rZpPOFRBefCA",
    "usPVA1hceoh2",
    "0tOkWGkVs3Uo",
    "BlPl2L0ybPkN",
    "mwRcnzOJcWGG"
   ],
   "provenance": [
    {
     "file_id": "1hugfZfRQ762fcbAXuSsxcWYBO-q16Ffq",
     "timestamp": 1676380655825
    },
    {
     "file_id": "1rIS3LLDgQ_aPDexPx3EQxFMrwUs7h2cZ",
     "timestamp": 1675782505060
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
