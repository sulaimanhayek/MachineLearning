{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1hugfZfRQ762fcbAXuSsxcWYBO-q16Ffq","timestamp":1676380655825},{"file_id":"1rIS3LLDgQ_aPDexPx3EQxFMrwUs7h2cZ","timestamp":1675782505060}],"collapsed_sections":["lhcT91UwAajV","_T0C9f64XN4z","rZpPOFRBefCA","usPVA1hceoh2","0tOkWGkVs3Uo","BlPl2L0ybPkN","mwRcnzOJcWGG"],"authorship_tag":"ABX9TyNxDYGLRilTxpOS1429RLpp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Objectives\n"],"metadata":{"id":"lhcT91UwAajV"}},{"cell_type":"markdown","source":["* To explore the convolutional and long short-term neural networks from Week 8 lecture and apply them for forecast of number of days of ground frost and snow based on other weather variables.\n","> **Remember**: It is your responsibility as a machine learning scientist to read documentations for each library function in the code to thoroughly understand what it is doing, how it serves the purpose highlighted in the code comments, and other parameters that could be set.\n"],"metadata":{"id":"xAV_surPsiiZ"}},{"cell_type":"markdown","source":["# Section 1 - Load new UK Met (60km, 2010-2022) data"],"metadata":{"id":"_T0C9f64XN4z"}},{"cell_type":"markdown","source":["Although the same UK Met dataset will be used, the format of the dataset for this week is different from that in Week 3.\n","\n","1. You need to first download the data before you can get started. Download from the Week 8 page for the module, on Canvas (see 'Week 8 Lab Dataset' on the page). The file you download will be named *curated_data_24month_2010-2022_nonans.csv*.\n","\n","2. Explore the dataset (see Section 1 of the Week 3 lab notebook for where to get started).\n","\n","3. Then, use the file menu in Google Colab to upload the file to your Colab directory. Once upload is complete, you should be able to see the file on the listed contents of your Colab directory.\n","\n","4. You can now run the code in the cell below to load the data."],"metadata":{"id":"O3qjCVVGXPhT"}},{"cell_type":"code","source":["import csv\n","import numpy\n","\n","\n","!ls  /content\n","\n","data_file_full_path = \"/content/curated_data_24month_2010-2022_nonans.csv\"\n","\n","data_as_list = []\n","\n","# load the dataset\n","with open(data_file_full_path) as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","\n","    row_count = 0\n","    for row in csv_reader:\n","\n","      if row_count > 0:\n","        data_as_list.append([float(val) for val in row])\n","      row_count += 1\n","data = numpy.array(data_as_list)\n","\n","# check its shape\n","print(\"\\n The dataset has shape: \"+str(data.shape))\n","\n","\n","# get features and labels from the data\n","# based on the objectives (see the Objectives section)\n","ground_frost_24_col = 27\n","hurs_1_to_23_col = numpy.arange(ground_frost_24_col+1, ground_frost_24_col+24)\n","psl_1_to_23_col = hurs_1_to_23_col + 23\n","sun_1_to_23_col = psl_1_to_23_col + 23\n","pv_1_to_23_col = sun_1_to_23_col + 23\n","wind_1_to_23_col = pv_1_to_23_col + 23\n","tas_1_to_23_col = wind_1_to_23_col + 23\n","snow_24_col = 97\n","rain_1_to_23_col = numpy.arange(snow_24_col+1, snow_24_col+24)\n","\n","# arrange to shape N x T X D\n","# where N = number of data instances\n","# T = number of timesteps\n","# D = number of variables\n","feats = numpy.stack((data[:, hurs_1_to_23_col], data[:, psl_1_to_23_col],\n","                     data[:, sun_1_to_23_col], data[:, pv_1_to_23_col],\n","                     data[:, wind_1_to_23_col], data[:, tas_1_to_23_col],\n","                     data[:, rain_1_to_23_col]))\n","feats = numpy.transpose(feats, (1, 2, 0))\n","ground_frost_label = data[:, ground_frost_24_col]\n","snow_label = data[:, snow_24_col]\n","\n","# check its shape\n","print(\"\\n The reshaped data has shape: \"+str(feats.shape))\n","\n","# take a peek\n","print(\"\\n A peek at the dataset features: \\n\"+str(feats))\n","print(\"\\n A peek at the ground frost labels: \\n\"+str(ground_frost_label))\n","print(\"\\n A peek at the snow labels: \\n\"+str(snow_label))\n"],"metadata":{"id":"ieQg8pLsYU1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 2 - Split into training, validation, and test sets"],"metadata":{"id":"rZpPOFRBefCA"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","all_ids = numpy.arange(0, feats.shape[0])\n","\n","random_seed = 1\n","\n","# First randomly split the data into 70:30 to get the training set\n","train_set_ids, rem_set_ids = train_test_split(all_ids, test_size=0.3, train_size=0.7,\n","                                 random_state=random_seed, shuffle=True)\n","\n","\n","# Then further split the remaining data 50:50 into validation and test sets\n","val_set_ids, test_set_ids = train_test_split(rem_set_ids, test_size=0.5, train_size=0.5,\n","                                 random_state=random_seed, shuffle=True)\n","\n","\n","train_ground_frost_labels = ground_frost_label[train_set_ids]\n","train_snow_labels = snow_label[train_set_ids]\n","\n","val_ground_frost_labels = ground_frost_label[val_set_ids]\n","val_snow_labels = snow_label[val_set_ids]\n","\n","test_ground_frost_labels = ground_frost_label[test_set_ids]\n","test_snow_labels = snow_label[test_set_ids]"],"metadata":{"id":"4alBjB0JegjD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 3 - Scale (i.e. normalize) the input data"],"metadata":{"id":"usPVA1hceoh2"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","\n","# note that unlike previous weeks\n","# a normalization is to 0-1 range here\n","scaled_feats = numpy.reshape(feats, (-1, feats.shape[2]))\n","scaler = MinMaxScaler()\n","scaler.fit(scaled_feats)\n","scaled_feats = scaler.transform(scaled_feats)\n","scaled_feats = numpy.reshape(scaled_feats, (feats.shape[0], feats.shape[1], feats.shape[2]))\n","print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_feats))\n","\n","scaled_train_data = scaled_feats[train_set_ids, :]\n","scaled_val_data = scaled_feats[val_set_ids, :]\n","scaled_test_data = scaled_feats[test_set_ids, :]"],"metadata":{"id":"BPCnbQmmeqSj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 4 - Train a convolutional neural network (CNN)"],"metadata":{"id":"0tOkWGkVs3Uo"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from copy import deepcopy\n","import sys\n","import matplotlib.pyplot as plt\n","\n","\n","\n","# Creating the network structure\n","# for a 3-layer CNN\n","class three_layer_CNN(nn.Module):\n","    def __init__(self,\n","                 hidden_layer_channel_sizes,\n","                 output_size, input_channel_size):\n","        super().__init__()\n","\n","        self.hidden_c1 = nn.Conv1d(input_channel_size, hidden_layer_channel_sizes[0], 6, stride=1)\n","        self.hidden_m1 = (nn.MaxPool1d(3, stride=1))\n","        self.hidden_c2 = nn.Conv1d(hidden_layer_channel_sizes[0], hidden_layer_channel_sizes[1], 4, stride=1)\n","        self.hidden_m2 = nn.MaxPool1d(3, stride=1)\n","        self.hidden_c3 = nn.Conv1d(hidden_layer_channel_sizes[1], hidden_layer_channel_sizes[2], 3, stride=1)\n","        self.hidden_m3 = nn.MaxPool1d(3, stride=1)\n","        self.output = nn.Linear(output_size[0], output_size[1])\n","        self.relu = nn.ReLU()\n","\n","\n","    def forward(self, inputs):\n","        out = self.hidden_c1(torch.permute(inputs, (0, 2, 1)))\n","        out = self.hidden_m1(self.relu(out))\n","        out = self.hidden_c2(out)\n","        out = self.hidden_m2(self.relu(out))\n","        out = self.hidden_c3(out)\n","        out = self.hidden_m3(self.relu(out))\n","        out = torch.reshape(out, (out.size(0), -1))\n","        out = self.output(self.relu(out))\n","        return out\n","\n","\n","\n","# a method for training a model\n","def train_model(model,\n","                train_set, val_set,\n","                epochs, learn_rate):\n","\n","  # Setting up the SGD optimizer for updating the model weights\n","  optimizer = optim.SGD(model.parameters(), lr=learn_rate)\n","\n","\n","  # Computing cross entropy loss against the training labels\n","  loss_function = nn.MSELoss()\n","\n","\n","\n","  best_model_mae = sys.float_info.max\n","  losses = []\n","\n","  # Iterating over the dataset at two different staages:\n","  # 1. Iterating over the batches in the dataset (inner for loop below)\n","  # One complete set of iteration through the dataset (i.e. having gone over\n","  # all batches in the dataset at least once) = One epoch\n","  # 2. Iterating over the specified numeber of epochs (outer for loop below)\n","  for epoch in range(0, epochs):\n","\n","      # Setting the model to training mode\n","      model.train()\n","\n","      if epoch == 0:  best_model = deepcopy(model)\n","\n","      for batch, (X_train, y_train) in enumerate(train_set):\n","\n","        # Zeroing out the `.grad` buffers,\n","        # otherwise on the backward pass we'll add the\n","        # new gradients to the old ones.\n","        optimizer.zero_grad()\n","\n","        # Computing the forward pass and then the loss\n","        train_pred = model.forward(X_train)\n","        train_loss = loss_function(train_pred, y_train)\n","        train_pred_numpy = train_pred.detach().numpy()\n","        train_mae = mean_absolute_error(y_train[:], train_pred_numpy[:])\n","\n","        # Computing the model parameters' gradients\n","        # and propagating the loss backwards through the network.\n","        train_loss.backward()\n","\n","        # Updating the model parameters using those gradients\n","        optimizer.step()\n","\n","      # Evaluating on the validation set\n","      model.eval()\n","      for batch, (X_val, y_val) in enumerate(val_set):\n","        val_pred = model.forward(X_val)\n","        val_loss = loss_function(val_pred, y_val)\n","        val_pred_numpy = val_pred.detach().numpy()\n","        val_mae = mean_absolute_error(y_val[:], val_pred_numpy[:])\n","\n","      if val_mae <= best_model_mae:\n","        best_model_mae = val_mae\n","        best_model = deepcopy(model)\n","        print('Found improvement in performance. New model saved.')\n","\n","      # How well the network does at the end of an epoch\n","      # is an indication of how well training is progressing\n","      print(\"epoch: {} - train loss: {:.4f} train mae: {:.2f} val loss: {:.4f} val mae: {:.2f}\".format(\n","          epoch,\n","          train_loss.item(),\n","          train_mae,\n","          val_loss.item(),\n","          val_mae))\n","\n","      losses.append([train_loss.item(), val_loss.item()])\n","\n","  return best_model, losses\n","\n","\n","# a method for evaluating a model\n","# including the metrics earlier prepared\n","# and a graph of the above loss per epoch\n","def evaluate_model(model,\n","                test_set,\n","                training_losses, plot=True, text=''):\n","\n","  model.eval()\n","  for batch, (X_test, y_test) in enumerate(test_set):\n","    test_pred = model.forward(X_test)\n","    test_pred_numpy = test_pred.detach().numpy()\n","    test_mae = mean_absolute_error(y_test[:], test_pred_numpy[:])\n","    print(\"\\n Mean absolute error on the test set{:s}: {:2.2f}\".format(text, test_mae))\n","    print()\n","\n","  if plot:\n","    fig, ax = plt.subplots()\n","    training_losses = numpy.array(training_losses)\n","    ax.plot(training_losses[:, 0], 'b-', label='training loss')\n","    ax.plot(training_losses[:, 1], 'k-', label='validation loss')\n","    plt.legend(loc='upper right')\n","    plt.xlabel(\"epochs\")\n","    plt.show\n","\n","\n","\n","\n","# A class for managing the data for training the model\n","class MetDataset(Dataset):\n","    def __init__(self, feats, labels):\n","        # Convert features from numpy arrays to PyTorch tensors\n","        self.feats = torch.tensor(feats, dtype=torch.float32)\n","\n","\n","        # Convert labels from numpy arrays to PyTorch tensors\n","        self.labels = torch.tensor(numpy.reshape(labels, (-1, 1)), dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","\n","        return self.feats[idx, :], self.labels[idx, :]\n"],"metadata":{"id":"iusYY-PFROe2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","# To ensure reproducibility\n","# for PyTorch operations that use random numbers internally\n","random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","numpy.random.seed(random_seed)\n","\n","# Create an instance of the CNN network\n","input_channel_size_cnn = 7\n","hidden_layer_sizes_cnn = [7, 30, 7]\n","output_sizes_cnn = [hidden_layer_sizes_cnn[2]*7, 1]\n","model_CNN = three_layer_CNN(hidden_layer_sizes_cnn, output_sizes_cnn, input_channel_size_cnn)\n","\n","\n","# Set hyperparameters\n","num_epochs_CNN = 100\n","learning_rate_CNN = 0.0005\n","batch_size = 50\n","\n","\n","# Set up the data loading by batches\n","# With the test and validation sets having only one batch\n","train_set = MetDataset(scaled_train_data, train_ground_frost_labels)\n","train_dataloader = DataLoader(train_set, batch_size=batch_size)\n","\n","val_set = MetDataset(scaled_val_data, val_ground_frost_labels)\n","val_dataloader = DataLoader(val_set, batch_size=len(val_set))\n","\n","test_set = MetDataset(scaled_test_data, test_ground_frost_labels)\n","test_dataloader = DataLoader(test_set, batch_size=len(test_set))\n","\n","\n","\n","# train the model\n","model_CNN, train_val_losses_CNN = train_model(model_CNN, train_dataloader,\n","                                                          val_dataloader,\n","                                                          num_epochs_CNN,\n","                                                          learning_rate_CNN)\n","\n","# evaluate the model\n","evaluate_model(model_CNN, test_dataloader, train_val_losses_CNN)\n","\n"],"metadata":{"id":"UjDHm6lxbAta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cBXvhWxVDD_h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 5 - Explore the CNN"],"metadata":{"id":"BlPl2L0ybPkN"}},{"cell_type":"markdown","source":["* For each layer of the CNN in Section 4, calculate the dimension of the output of the layer. (Hint: See Week 8 lecture slides for the formulae for computing the output dimension of a convolutional layer. The same formula applies to pooling layers.)\n","\n","* Based on your calculations above and the structure of the CNN, how many parameters does the model have? How does this compare with the number of parameters if each convolutional layer was instead a fully connected layer.\n","\n","* Perform data augmentation (see Week 5 lecture), e.g.\n","  * dropout along the time dimension,\n","  * dropout along the variable dimension,\n","  * random noise along the time dimension.\n","\n","* How does the augmentation affect generalisation of the model?"],"metadata":{"id":"1ndUxf-Hbbov"}},{"cell_type":"markdown","source":["# Section 6 - Train a long short-term memory neural network (LSTMNN)"],"metadata":{"id":"mwRcnzOJcWGG"}},{"cell_type":"markdown","source":["* Create and train a three-layer LSTMNN model (see https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html for the documentation for the PyTorch LSTM layer)\n","\n","* How many weights does the LSTMNN have?\n","\n","* How does your model perform in comparison with the CNN in Section 4?"],"metadata":{"id":"hndEuTUQbta1"}}]}