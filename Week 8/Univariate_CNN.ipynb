{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate CNN Models\n",
    "Although traditionally developed for two-dimensional image data, CNNs can be used to model univariate time series forecasting problems.\n",
    "\n",
    "Univariate time series are datasets comprised of a single series of observations with a temporal ordering and a model is required to learn from the series of past observations to predict the next value in the sequence.\n",
    "\n",
    "We'll need to do:\n",
    "1. Data Preparation\n",
    "2. CNN Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation\n",
    "Before a univariate series can be modeled, it must be prepared.\n",
    "\n",
    "The CNN model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the model can learn.\n",
    "\n",
    "Consider a given univariate sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[10, 20, 30, 40, 50, 60, 70, 80, 90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,\t\t\t\ty\n",
    "10, 20, 30\t\t40\n",
    "20, 30, 40\t\t50\n",
    "30, 40, 50\t\t60\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] 40\n",
      "[20 30 40] 50\n",
      "[30 40 50] 60\n",
      "[40 50 60] 70\n",
      "[50 60 70] 80\n",
      "[60 70 80] 90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# univariate data preparation\n",
    "from numpy import array\n",
    " \n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model\n",
    "A one-dimensional CNN is a CNN model that has a convolutional hidden layer that operates over a 1D sequence. This is followed by perhaps a second convolutional layer in some cases, such as very long input sequences, and then a pooling layer whose job it is to distill the output of the convolutional layer to the most salient elements.\n",
    "\n",
    "The convolutional and pooling layers are followed by a dense fully connected layer that interprets the features extracted by the convolutional part of the model. A flatten layer is used between the convolutional layers and the dense layer to reduce the feature maps to a single one-dimensional vector.\n",
    "\n",
    "We can define a 1D CNN Model for univariate time series forecasting as follows.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps and the number of features.\n",
    "\n",
    "We are working with a univariate series, so the number of features is one, for one variable.\n",
    "\n",
    "The number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function.\n",
    "\n",
    "The input shape for each sample is specified in the input_shape argument on the definition of the first hidden layer.\n",
    "\n",
    "We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape:\n",
    "\n",
    "[samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our split_sequence() function in the previous section outputs the X with the shape [samples, timesteps], so we can easily reshape it to have an additional dimension for the one feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN does not actually view the data as having time steps, instead, it is treated as a sequence over which convolutional read operations can be performed, like a one-dimensional image.\n",
    "\n",
    "In this example, we define a convolutional layer with 64 filter maps and a kernel size of 2. This is followed by a max pooling layer and a dense layer to interpret the input feature. An output layer is specified that predicts a single numerical value.\n",
    "\n",
    "The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or ‘mse‘, loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.60421752929688\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "n_steps = 3\n",
    "\n",
    "# Prepare data\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))  # PyTorch expects (batch, channel, seq_len)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define model\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=2)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * ((n_steps - 2 + 1) // 2), 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = CNN1D()\n",
    "\n",
    "# Compile model\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Fit model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predict\n",
    "x_input = np.array([70, 80, 90]).reshape((1, 1, n_steps))\n",
    "x_input_tensor = torch.tensor(x_input, dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat = model(x_input_tensor)\n",
    "print(yhat.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 112.1747\n",
      "Epoch 10: Loss = 1.9400\n",
      "Epoch 20: Loss = 6.6606\n",
      "Epoch 30: Loss = 3.1378\n",
      "Epoch 40: Loss = 1.8552\n",
      "Epoch 50: Loss = 2.0000\n",
      "Epoch 60: Loss = 1.7135\n",
      "Epoch 70: Loss = 1.6185\n",
      "Epoch 80: Loss = 1.5866\n",
      "Epoch 90: Loss = 1.5448\n",
      "\n",
      "Sample predictions for 2026:\n",
      "tensor([])\n",
      "True values:\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---- 1. Generate Synthetic Data ----\n",
    "np.random.seed(42)\n",
    "\n",
    "years = list(range(2018, 2026))  # Include 2026 for prediction\n",
    "months = list(range(1, 13))\n",
    "countries = [f\"Country_{i}\" for i in range(10)]\n",
    "items = [\"banana\", \"apple\", \"orange\"]\n",
    "\n",
    "records = []\n",
    "for country in countries:\n",
    "    for item in items:\n",
    "        for year in years:\n",
    "            monthly_feature = np.random.rand(12) * 10  # e.g., rainfall or temperature\n",
    "            yield_value = 5 + 0.1 * monthly_feature.sum() + np.random.normal(0, 1)\n",
    "            records.append({\n",
    "                \"country\": country,\n",
    "                \"item\": item,\n",
    "                \"year\": year,\n",
    "                **{f\"month_{i+1}\": monthly_feature[i] for i in range(12)},\n",
    "                \"yield\": yield_value\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# ---- 2. Prepare Data ----\n",
    "le_country = LabelEncoder()\n",
    "le_item = LabelEncoder()\n",
    "\n",
    "df[\"country_idx\"] = le_country.fit_transform(df[\"country\"])\n",
    "df[\"item_idx\"] = le_item.fit_transform(df[\"item\"])\n",
    "\n",
    "# Normalize year as numeric input\n",
    "df[\"year_norm\"] = (df[\"year\"] - df[\"year\"].min()) / (df[\"year\"].max() - df[\"year\"].min())\n",
    "\n",
    "# Train/test split (predict 2026 only)\n",
    "df_train = df[df.year < 2026]\n",
    "df_test = df[df.year == 2026]\n",
    "\n",
    "def to_tensor(df_part):\n",
    "    X_seq = df_part[[f\"month_{i}\" for i in range(1, 13)]].values\n",
    "    X_seq = torch.tensor(X_seq, dtype=torch.float32).unsqueeze(1)  # (B, 1, 12)\n",
    "    country = torch.tensor(df_part[\"country_idx\"].values, dtype=torch.long)\n",
    "    item = torch.tensor(df_part[\"item_idx\"].values, dtype=torch.long)\n",
    "    year_norm = torch.tensor(df_part[\"year_norm\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "    y = torch.tensor(df_part[\"yield\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "    return X_seq, country, item, year_norm, y\n",
    "\n",
    "X_train, c_train, i_train, y_train_norm, y_train = to_tensor(df_train)\n",
    "X_test, c_test, i_test, y_test_norm, y_test = to_tensor(df_test)\n",
    "\n",
    "# ---- 3. Define Model ----\n",
    "class YieldCNN(nn.Module):\n",
    "    def __init__(self, num_countries, num_items):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.emb_country = nn.Embedding(num_countries, 8)\n",
    "        self.emb_item = nn.Embedding(num_items, 8)\n",
    "        self.fc1 = nn.Linear(64 * 5 + 8 + 8 + 1, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x_seq, c, i, year_float):\n",
    "        x = torch.relu(self.conv1(x_seq))  # (B, 64, 10)\n",
    "        x = self.pool(x)                   # (B, 64, 5)\n",
    "        x = self.flatten(x)                # (B, 320)\n",
    "        emb_c = self.emb_country(c)\n",
    "        emb_i = self.emb_item(i)\n",
    "        x = torch.cat([x, emb_c, emb_i, year_float], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# ---- 4. Train Model ----\n",
    "model = YieldCNN(num_countries=len(le_country.classes_),\n",
    "                 num_items=len(le_item.classes_))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train, c_train, i_train, y_train_norm)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# ---- 5. Predict 2026 ----\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_2026 = model(X_test, c_test, i_test, y_test_norm)\n",
    "    print(\"\\nSample predictions for 2026:\")\n",
    "    print(y_pred_2026[:5].squeeze())\n",
    "    print(\"True values:\")\n",
    "    print(y_test[:5].squeeze())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
